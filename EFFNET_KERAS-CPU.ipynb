{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import pandas as pd\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import numpy as np \n",
    "import itertools\n",
    "import tensorflow.keras\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPool2D \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization, Flatten\n",
    "from tensorflow.keras import applications \n",
    "from keras.utils.np_utils import to_categorical \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "import efficientnet.keras as enet\n",
    "import keras_applications as app\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Model\n",
    "import keras\n",
    "#%matplotlib inline\n",
    "import math \n",
    "import datetime\n",
    "import time\n",
    "CUDA_VISIBLE_DEVICES = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Default dimensions we found online\n",
    "img_width, img_height = 224, 224 \n",
    " \n",
    "#Create a bottleneck file\n",
    "top_model_weights_path = \"bottleneck_fc_model_50_50.h5\"\n",
    "# loading up our datasets\n",
    "train_data_dir = '/media/hades/DISK 1 - Drive 2/Fish/Test7/Train' \n",
    "validation_data_dir = '/media/hades/DISK 1 - Drive 2/Fish/Test7/Validate' \n",
    "test_data_dir = '/media/hades/DISK 1 - Drive 2/Fish/Test7/Test'\n",
    " \n",
    "# number of epochs to train top model \n",
    "epochs = 50 #this has been changed after multiple model run \n",
    "# batch size used by flow_from_directory and predict_generator \n",
    "batch_size = 50 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAS\n",
      "HADOH\n"
     ]
    }
   ],
   "source": [
    "import efficientnet.keras as enet\n",
    "\n",
    "#Loading vgc16 model\n",
    "print(\"HAS\")\n",
    "#vgg16 = app.efficientnet.EfficientNetB7(utils = tensorflow.keras.utils, models=tensorflow.keras.models, layers = tensorflow.keras.layers, backend = tensorflow.keras.backend, include_top=False, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "#vgg16 = app.vgg16.VGG16(utils = keras.utils, models=keras.models, layers = keras.layers, backend = keras.backend, include_top=False, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "#vgg16 = applications.VGG16(include_top=False, weights=’imagenet’)\n",
    "#vgg16 = app.mobilenet_v3.MobileNetV3(utils = keras.utils, models=keras.models, layers = keras.layers, backend = keras.backend, include_top=False, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "#vgg16 = app.resnet50.ResNet50(utils = keras.utils, models=keras.models, layers = keras.layers, backend = keras.backend, include_top=False, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "vgg16=app.mobilenet.MobileNet(utils = tensorflow.keras.utils, models=tensorflow.keras.models, layers = tensorflow.keras.layers, backend = tensorflow.keras.backend, include_top=True, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "# datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "#       )\n",
    "print(\"HADOH\")\n",
    "#needed to create the bottleneck .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenet_1.00_224\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 225, 225, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_preds (Conv2D)          (None, 1, 1, 1000)        1025000   \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "act_softmax (Activation)     (None, 1000)              0         \n",
      "=================================================================\n",
      "Total params: 4,253,864\n",
      "Trainable params: 4,231,976\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "\n",
    "#plot_model(model, to_file='toplayers.png', show_shapes=True, show_layer_names=True)\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASD1\n",
      "Found 5150 images belonging to 55 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hades/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n",
      "/home/hades/.local/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0:02:15.559115\n"
     ]
    }
   ],
   "source": [
    "#__this can take an hour and half to run so only run it once. \n",
    "#once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__\n",
    "start = datetime.datetime.now()\n",
    "print(\"ASD1\")\n",
    "generator = datagen.flow_from_directory( \n",
    "    train_data_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode=None, \n",
    "    shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator.filenames) \n",
    "num_classes = len(generator.class_indices) \n",
    " \n",
    "predict_size_train = int(math.ceil(nb_train_samples / batch_size)) \n",
    " \n",
    "bottleneck_features_train = vgg16.predict_generator(generator, predict_size_train) \n",
    "vgg16.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "np.save('bottleneck_features_train_50_50.npy', bottleneck_features_train)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 537 images belonging to 55 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hades/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0:00:14.431992\n"
     ]
    }
   ],
   "source": [
    "#__this can take an hour and half to run so only run it once. \n",
    "#once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__\n",
    "start = datetime.datetime.now()\n",
    " \n",
    "generator = datagen.flow_from_directory( \n",
    "    test_data_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode=None, \n",
    "    shuffle=False) \n",
    " \n",
    "nb_validation_samples = len(generator.filenames) \n",
    "num_classes = len(generator.class_indices) \n",
    " \n",
    "predict_size_validation = int(math.ceil(nb_validation_samples / batch_size)) \n",
    " \n",
    "bottleneck_features_validation = vgg16.predict_generator(generator, predict_size_validation) \n",
    " \n",
    "np.save('bottleneck_features_validation_50_50.npy', bottleneck_features_validation)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5150 images belonging to 55 classes.\n"
     ]
    }
   ],
   "source": [
    "#training data\n",
    "generator_top = datagen.flow_from_directory( \n",
    "   train_data_dir, \n",
    "   target_size=(img_width, img_height), \n",
    "   batch_size=batch_size, \n",
    "   class_mode='categorical', \n",
    "   shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator_top.filenames) \n",
    "num_classes = len(generator_top.class_indices) \n",
    " \n",
    "# load the bottleneck features saved earlier \n",
    "train_data = np.load('bottleneck_features_train_50_50.npy') \n",
    " \n",
    "# get the class labels for the training data, in the original order \n",
    "train_labels = generator_top.classes \n",
    " \n",
    "# convert the training labels to categorical vectors \n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 537 images belonging to 55 classes.\n"
     ]
    }
   ],
   "source": [
    "#training data\n",
    "generator_top = datagen.flow_from_directory( \n",
    "   test_data_dir, \n",
    "   target_size=(img_width, img_height), \n",
    "   batch_size=batch_size, \n",
    "   class_mode='categorical', \n",
    "   shuffle=False) \n",
    " \n",
    "nb_validation_samples = len(generator_top.filenames) \n",
    "num_classes = len(generator_top.class_indices) \n",
    " \n",
    "# load the bottleneck features saved earlier \n",
    "validation_data = np.load('bottleneck_features_validation_50_50.npy') \n",
    " \n",
    "# get the class labels for the training data, in the original order \n",
    "validation_labels = generator_top.classes \n",
    " \n",
    "# convert the training labels to categorical vectors \n",
    "validation_labels = to_categorical(validation_labels, num_classes=num_classes)\n",
    "\n",
    "#validation_data = validation_data.reshape((-1, 224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_file=generator_top.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swish defination\n",
    "from tensorflow.keras.backend import sigmoid\n",
    "\n",
    "class SwishActivation(Activation):\n",
    "    \n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(SwishActivation, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'swish_act'\n",
    "\n",
    "def swish_act(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation\n",
    "get_custom_objects().update({'swish_act': SwishActivation(swish_act)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "103/103 [==============================] - 1s 4ms/step - loss: 3.9972 - accuracy: 0.0832 - val_loss: 3.9893 - val_accuracy: 0.1006\n",
      "Epoch 2/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.9857 - accuracy: 0.1193 - val_loss: 3.9784 - val_accuracy: 0.1415\n",
      "Epoch 3/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.9735 - accuracy: 0.1556 - val_loss: 3.9677 - val_accuracy: 0.1769\n",
      "Epoch 4/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.9639 - accuracy: 0.1750 - val_loss: 3.9570 - val_accuracy: 0.1825\n",
      "Epoch 5/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.9530 - accuracy: 0.1721 - val_loss: 3.9464 - val_accuracy: 0.1918\n",
      "Epoch 6/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.9421 - accuracy: 0.1788 - val_loss: 3.9360 - val_accuracy: 0.1974\n",
      "Epoch 7/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.9292 - accuracy: 0.1866 - val_loss: 3.9255 - val_accuracy: 0.2142\n",
      "Epoch 8/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.9184 - accuracy: 0.1976 - val_loss: 3.9152 - val_accuracy: 0.2179\n",
      "Epoch 9/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.9083 - accuracy: 0.1997 - val_loss: 3.9051 - val_accuracy: 0.2179\n",
      "Epoch 10/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.8977 - accuracy: 0.2043 - val_loss: 3.8950 - val_accuracy: 0.2439\n",
      "Epoch 11/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.8884 - accuracy: 0.2056 - val_loss: 3.8849 - val_accuracy: 0.2495\n",
      "Epoch 12/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.8783 - accuracy: 0.2102 - val_loss: 3.8750 - val_accuracy: 0.2551\n",
      "Epoch 13/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.8654 - accuracy: 0.2236 - val_loss: 3.8651 - val_accuracy: 0.2588\n",
      "Epoch 14/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.8553 - accuracy: 0.2187 - val_loss: 3.8554 - val_accuracy: 0.2737\n",
      "Epoch 15/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.8449 - accuracy: 0.2292 - val_loss: 3.8457 - val_accuracy: 0.2793\n",
      "Epoch 16/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.8344 - accuracy: 0.2280 - val_loss: 3.8361 - val_accuracy: 0.2849\n",
      "Epoch 17/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.8236 - accuracy: 0.2438 - val_loss: 3.8266 - val_accuracy: 0.2886\n",
      "Epoch 18/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.8129 - accuracy: 0.2515 - val_loss: 3.8171 - val_accuracy: 0.2924\n",
      "Epoch 19/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.8043 - accuracy: 0.2468 - val_loss: 3.8077 - val_accuracy: 0.2961\n",
      "Epoch 20/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.7962 - accuracy: 0.2464 - val_loss: 3.7984 - val_accuracy: 0.2961\n",
      "Epoch 21/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.7857 - accuracy: 0.2499 - val_loss: 3.7892 - val_accuracy: 0.2942\n",
      "Epoch 22/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.7762 - accuracy: 0.2569 - val_loss: 3.7800 - val_accuracy: 0.2961\n",
      "Epoch 23/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.7665 - accuracy: 0.2520 - val_loss: 3.7710 - val_accuracy: 0.2886\n",
      "Epoch 24/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.7590 - accuracy: 0.2511 - val_loss: 3.7620 - val_accuracy: 0.2886\n",
      "Epoch 25/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.7460 - accuracy: 0.2470 - val_loss: 3.7530 - val_accuracy: 0.2868\n",
      "Epoch 26/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.7342 - accuracy: 0.2571 - val_loss: 3.7442 - val_accuracy: 0.2849\n",
      "Epoch 27/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.7314 - accuracy: 0.2560 - val_loss: 3.7354 - val_accuracy: 0.2831\n",
      "Epoch 28/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.7181 - accuracy: 0.2561 - val_loss: 3.7267 - val_accuracy: 0.2831\n",
      "Epoch 29/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.7065 - accuracy: 0.2605 - val_loss: 3.7181 - val_accuracy: 0.2831\n",
      "Epoch 30/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.7042 - accuracy: 0.2541 - val_loss: 3.7095 - val_accuracy: 0.2831\n",
      "Epoch 31/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.6896 - accuracy: 0.2582 - val_loss: 3.7011 - val_accuracy: 0.2812\n",
      "Epoch 32/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.6889 - accuracy: 0.2586 - val_loss: 3.6926 - val_accuracy: 0.2775\n",
      "Epoch 33/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.6760 - accuracy: 0.2503 - val_loss: 3.6843 - val_accuracy: 0.2756\n",
      "Epoch 34/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.6665 - accuracy: 0.2521 - val_loss: 3.6760 - val_accuracy: 0.2756\n",
      "Epoch 35/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.6537 - accuracy: 0.2572 - val_loss: 3.6678 - val_accuracy: 0.2756\n",
      "Epoch 36/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.6508 - accuracy: 0.2550 - val_loss: 3.6596 - val_accuracy: 0.2756\n",
      "Epoch 37/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.6424 - accuracy: 0.2505 - val_loss: 3.6515 - val_accuracy: 0.2756\n",
      "Epoch 38/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.6247 - accuracy: 0.2644 - val_loss: 3.6435 - val_accuracy: 0.2775\n",
      "Epoch 39/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.6238 - accuracy: 0.2545 - val_loss: 3.6356 - val_accuracy: 0.2793\n",
      "Epoch 40/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.6089 - accuracy: 0.2618 - val_loss: 3.6277 - val_accuracy: 0.2775\n",
      "Epoch 41/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.6152 - accuracy: 0.2453 - val_loss: 3.6199 - val_accuracy: 0.2775\n",
      "Epoch 42/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.5994 - accuracy: 0.2566 - val_loss: 3.6122 - val_accuracy: 0.2756\n",
      "Epoch 43/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.5951 - accuracy: 0.2465 - val_loss: 3.6044 - val_accuracy: 0.2775\n",
      "Epoch 44/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.5862 - accuracy: 0.2524 - val_loss: 3.5968 - val_accuracy: 0.2756\n",
      "Epoch 45/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.5729 - accuracy: 0.2590 - val_loss: 3.5893 - val_accuracy: 0.2793\n",
      "Epoch 46/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.5625 - accuracy: 0.2554 - val_loss: 3.5817 - val_accuracy: 0.2737\n",
      "Epoch 47/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.5539 - accuracy: 0.2540 - val_loss: 3.5743 - val_accuracy: 0.2682\n",
      "Epoch 48/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.5427 - accuracy: 0.2537 - val_loss: 3.5670 - val_accuracy: 0.2682\n",
      "Epoch 49/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.5468 - accuracy: 0.2452 - val_loss: 3.5596 - val_accuracy: 0.2682\n",
      "Epoch 50/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.5210 - accuracy: 0.2667 - val_loss: 3.5523 - val_accuracy: 0.2700\n",
      "Epoch 51/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.5289 - accuracy: 0.2502 - val_loss: 3.5451 - val_accuracy: 0.2700\n",
      "Epoch 52/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.5240 - accuracy: 0.2388 - val_loss: 3.5380 - val_accuracy: 0.2700\n",
      "Epoch 53/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.5094 - accuracy: 0.2582 - val_loss: 3.5308 - val_accuracy: 0.2700\n",
      "Epoch 54/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.5011 - accuracy: 0.2547 - val_loss: 3.5238 - val_accuracy: 0.2700\n",
      "Epoch 55/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.4976 - accuracy: 0.2528 - val_loss: 3.5167 - val_accuracy: 0.2700\n",
      "Epoch 56/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4850 - accuracy: 0.2600 - val_loss: 3.5098 - val_accuracy: 0.2700\n",
      "Epoch 57/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.4852 - accuracy: 0.2525 - val_loss: 3.5029 - val_accuracy: 0.2700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.4810 - accuracy: 0.2460 - val_loss: 3.4961 - val_accuracy: 0.2700\n",
      "Epoch 59/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.4728 - accuracy: 0.2465 - val_loss: 3.4893 - val_accuracy: 0.2700\n",
      "Epoch 60/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4649 - accuracy: 0.2472 - val_loss: 3.4826 - val_accuracy: 0.2700\n",
      "Epoch 61/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.4514 - accuracy: 0.2548 - val_loss: 3.4758 - val_accuracy: 0.2719\n",
      "Epoch 62/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4417 - accuracy: 0.2522 - val_loss: 3.4692 - val_accuracy: 0.2737\n",
      "Epoch 63/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4400 - accuracy: 0.2608 - val_loss: 3.4626 - val_accuracy: 0.2737\n",
      "Epoch 64/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4352 - accuracy: 0.2598 - val_loss: 3.4561 - val_accuracy: 0.2737\n",
      "Epoch 65/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4252 - accuracy: 0.2614 - val_loss: 3.4496 - val_accuracy: 0.2737\n",
      "Epoch 66/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.4241 - accuracy: 0.2562 - val_loss: 3.4432 - val_accuracy: 0.2737\n",
      "Epoch 67/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.4066 - accuracy: 0.2591 - val_loss: 3.4368 - val_accuracy: 0.2737\n",
      "Epoch 68/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.4020 - accuracy: 0.2570 - val_loss: 3.4305 - val_accuracy: 0.2737\n",
      "Epoch 69/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3942 - accuracy: 0.2608 - val_loss: 3.4242 - val_accuracy: 0.2737\n",
      "Epoch 70/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4017 - accuracy: 0.2443 - val_loss: 3.4179 - val_accuracy: 0.2756\n",
      "Epoch 71/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.3831 - accuracy: 0.2454 - val_loss: 3.4117 - val_accuracy: 0.2756\n",
      "Epoch 72/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.3781 - accuracy: 0.2541 - val_loss: 3.4056 - val_accuracy: 0.2756\n",
      "Epoch 73/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.3848 - accuracy: 0.2458 - val_loss: 3.3994 - val_accuracy: 0.2756\n",
      "Epoch 74/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.3694 - accuracy: 0.2486 - val_loss: 3.3933 - val_accuracy: 0.2756\n",
      "Epoch 75/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.3638 - accuracy: 0.2528 - val_loss: 3.3874 - val_accuracy: 0.2775\n",
      "Epoch 76/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3501 - accuracy: 0.2509 - val_loss: 3.3813 - val_accuracy: 0.2775\n",
      "Epoch 77/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3540 - accuracy: 0.2519 - val_loss: 3.3754 - val_accuracy: 0.2775\n",
      "Epoch 78/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.3454 - accuracy: 0.2511 - val_loss: 3.3695 - val_accuracy: 0.2775\n",
      "Epoch 79/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3332 - accuracy: 0.2567 - val_loss: 3.3636 - val_accuracy: 0.2775\n",
      "Epoch 80/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.3255 - accuracy: 0.2598 - val_loss: 3.3578 - val_accuracy: 0.2775\n",
      "Epoch 81/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.3154 - accuracy: 0.2589 - val_loss: 3.3520 - val_accuracy: 0.2775\n",
      "Epoch 82/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.3273 - accuracy: 0.2517 - val_loss: 3.3463 - val_accuracy: 0.2775\n",
      "Epoch 83/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.3192 - accuracy: 0.2517 - val_loss: 3.3405 - val_accuracy: 0.2775\n",
      "Epoch 84/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.2913 - accuracy: 0.2583 - val_loss: 3.3348 - val_accuracy: 0.2775\n",
      "Epoch 85/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.3073 - accuracy: 0.2477 - val_loss: 3.3292 - val_accuracy: 0.2793\n",
      "Epoch 86/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.2944 - accuracy: 0.2588 - val_loss: 3.3237 - val_accuracy: 0.2793\n",
      "Epoch 87/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.3003 - accuracy: 0.2523 - val_loss: 3.3181 - val_accuracy: 0.2793\n",
      "Epoch 88/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.2847 - accuracy: 0.2547 - val_loss: 3.3126 - val_accuracy: 0.2793\n",
      "Epoch 89/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.2711 - accuracy: 0.2556 - val_loss: 3.3071 - val_accuracy: 0.2831\n",
      "Epoch 90/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.2697 - accuracy: 0.2575 - val_loss: 3.3016 - val_accuracy: 0.2831\n",
      "Epoch 91/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.2795 - accuracy: 0.2449 - val_loss: 3.2962 - val_accuracy: 0.2831\n",
      "Epoch 92/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.2679 - accuracy: 0.2581 - val_loss: 3.2908 - val_accuracy: 0.2831\n",
      "Epoch 93/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.2738 - accuracy: 0.2418 - val_loss: 3.2855 - val_accuracy: 0.2831\n",
      "Epoch 94/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.2455 - accuracy: 0.2658 - val_loss: 3.2801 - val_accuracy: 0.2831\n",
      "Epoch 95/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2464 - accuracy: 0.2578 - val_loss: 3.2749 - val_accuracy: 0.2831\n",
      "Epoch 96/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.2599 - accuracy: 0.2507 - val_loss: 3.2696 - val_accuracy: 0.2849\n",
      "Epoch 97/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2489 - accuracy: 0.2467 - val_loss: 3.2644 - val_accuracy: 0.2849\n",
      "Epoch 98/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.2331 - accuracy: 0.2508 - val_loss: 3.2592 - val_accuracy: 0.2849\n",
      "Epoch 99/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.2334 - accuracy: 0.2613 - val_loss: 3.2540 - val_accuracy: 0.2849\n",
      "Epoch 100/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.2305 - accuracy: 0.2541 - val_loss: 3.2489 - val_accuracy: 0.2849\n",
      "Epoch 101/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.2064 - accuracy: 0.2638 - val_loss: 3.2438 - val_accuracy: 0.2868\n",
      "Epoch 102/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2049 - accuracy: 0.2607 - val_loss: 3.2387 - val_accuracy: 0.2868\n",
      "Epoch 103/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.2105 - accuracy: 0.2597 - val_loss: 3.2337 - val_accuracy: 0.2868\n",
      "Epoch 104/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.1961 - accuracy: 0.2615 - val_loss: 3.2287 - val_accuracy: 0.2868\n",
      "Epoch 105/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2065 - accuracy: 0.2615 - val_loss: 3.2237 - val_accuracy: 0.2868\n",
      "Epoch 106/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.1855 - accuracy: 0.2585 - val_loss: 3.2188 - val_accuracy: 0.2868\n",
      "Epoch 107/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.2023 - accuracy: 0.2472 - val_loss: 3.2139 - val_accuracy: 0.2868\n",
      "Epoch 108/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.2022 - accuracy: 0.2449 - val_loss: 3.2090 - val_accuracy: 0.2868\n",
      "Epoch 109/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.1849 - accuracy: 0.2531 - val_loss: 3.2042 - val_accuracy: 0.2868\n",
      "Epoch 110/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.1652 - accuracy: 0.2610 - val_loss: 3.1993 - val_accuracy: 0.2868\n",
      "Epoch 111/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.1636 - accuracy: 0.2595 - val_loss: 3.1946 - val_accuracy: 0.2868\n",
      "Epoch 112/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.1698 - accuracy: 0.2599 - val_loss: 3.1899 - val_accuracy: 0.2868\n",
      "Epoch 113/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.1664 - accuracy: 0.2615 - val_loss: 3.1852 - val_accuracy: 0.2886\n",
      "Epoch 114/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.1430 - accuracy: 0.2646 - val_loss: 3.1804 - val_accuracy: 0.2868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.1383 - accuracy: 0.2694 - val_loss: 3.1757 - val_accuracy: 0.2905\n",
      "Epoch 116/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.1449 - accuracy: 0.2624 - val_loss: 3.1710 - val_accuracy: 0.2905\n",
      "Epoch 117/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.1465 - accuracy: 0.2561 - val_loss: 3.1664 - val_accuracy: 0.2905\n",
      "Epoch 118/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.1452 - accuracy: 0.2604 - val_loss: 3.1618 - val_accuracy: 0.2942\n",
      "Epoch 119/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.1281 - accuracy: 0.2643 - val_loss: 3.1572 - val_accuracy: 0.2961\n",
      "Epoch 120/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.1340 - accuracy: 0.2656 - val_loss: 3.1527 - val_accuracy: 0.2998\n",
      "Epoch 121/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.1011 - accuracy: 0.2689 - val_loss: 3.1481 - val_accuracy: 0.2998\n",
      "Epoch 122/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.1164 - accuracy: 0.2665 - val_loss: 3.1436 - val_accuracy: 0.2998\n",
      "Epoch 123/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.1099 - accuracy: 0.2617 - val_loss: 3.1391 - val_accuracy: 0.2998\n",
      "Epoch 124/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.1122 - accuracy: 0.2684 - val_loss: 3.1345 - val_accuracy: 0.2998\n",
      "Epoch 125/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.0954 - accuracy: 0.2711 - val_loss: 3.1301 - val_accuracy: 0.3017\n",
      "Epoch 126/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.1168 - accuracy: 0.2602 - val_loss: 3.1257 - val_accuracy: 0.3017\n",
      "Epoch 127/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.1076 - accuracy: 0.2682 - val_loss: 3.1213 - val_accuracy: 0.3017\n",
      "Epoch 128/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.0855 - accuracy: 0.2765 - val_loss: 3.1169 - val_accuracy: 0.3017\n",
      "Epoch 129/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.0872 - accuracy: 0.2679 - val_loss: 3.1125 - val_accuracy: 0.3017\n",
      "Epoch 130/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0780 - accuracy: 0.2724 - val_loss: 3.1082 - val_accuracy: 0.3035\n",
      "Epoch 131/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.0783 - accuracy: 0.2665 - val_loss: 3.1039 - val_accuracy: 0.3035\n",
      "Epoch 132/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.0858 - accuracy: 0.2649 - val_loss: 3.0996 - val_accuracy: 0.3035\n",
      "Epoch 133/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.0699 - accuracy: 0.2724 - val_loss: 3.0953 - val_accuracy: 0.3017\n",
      "Epoch 134/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.0739 - accuracy: 0.2637 - val_loss: 3.0910 - val_accuracy: 0.3035\n",
      "Epoch 135/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.0484 - accuracy: 0.2767 - val_loss: 3.0868 - val_accuracy: 0.3035\n",
      "Epoch 136/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0453 - accuracy: 0.2810 - val_loss: 3.0826 - val_accuracy: 0.3035\n",
      "Epoch 137/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0594 - accuracy: 0.2730 - val_loss: 3.0784 - val_accuracy: 0.3035\n",
      "Epoch 138/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.0508 - accuracy: 0.2706 - val_loss: 3.0742 - val_accuracy: 0.3054\n",
      "Epoch 139/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.0655 - accuracy: 0.2643 - val_loss: 3.0700 - val_accuracy: 0.3054\n",
      "Epoch 140/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.0399 - accuracy: 0.2760 - val_loss: 3.0659 - val_accuracy: 0.3054\n",
      "Epoch 141/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0251 - accuracy: 0.2748 - val_loss: 3.0617 - val_accuracy: 0.3054\n",
      "Epoch 142/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0413 - accuracy: 0.2734 - val_loss: 3.0576 - val_accuracy: 0.3073\n",
      "Epoch 143/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.0282 - accuracy: 0.2824 - val_loss: 3.0535 - val_accuracy: 0.3073\n",
      "Epoch 144/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0068 - accuracy: 0.2825 - val_loss: 3.0495 - val_accuracy: 0.3091\n",
      "Epoch 145/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0149 - accuracy: 0.2784 - val_loss: 3.0454 - val_accuracy: 0.3091\n",
      "Epoch 146/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.0440 - accuracy: 0.2709 - val_loss: 3.0414 - val_accuracy: 0.3110\n",
      "Epoch 147/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0077 - accuracy: 0.2754 - val_loss: 3.0374 - val_accuracy: 0.3110\n",
      "Epoch 148/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0178 - accuracy: 0.2764 - val_loss: 3.0334 - val_accuracy: 0.3110\n",
      "Epoch 149/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.0298 - accuracy: 0.2714 - val_loss: 3.0294 - val_accuracy: 0.3110\n",
      "Epoch 150/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.0090 - accuracy: 0.2729 - val_loss: 3.0254 - val_accuracy: 0.3128\n",
      "Epoch 151/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0081 - accuracy: 0.2719 - val_loss: 3.0215 - val_accuracy: 0.3128\n",
      "Epoch 152/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9800 - accuracy: 0.2821 - val_loss: 3.0175 - val_accuracy: 0.3128\n",
      "Epoch 153/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 3.0012 - accuracy: 0.2639 - val_loss: 3.0136 - val_accuracy: 0.3128\n",
      "Epoch 154/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9756 - accuracy: 0.2856 - val_loss: 3.0097 - val_accuracy: 0.3128\n",
      "Epoch 155/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9808 - accuracy: 0.2797 - val_loss: 3.0058 - val_accuracy: 0.3128\n",
      "Epoch 156/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9935 - accuracy: 0.2734 - val_loss: 3.0019 - val_accuracy: 0.3128\n",
      "Epoch 157/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9753 - accuracy: 0.2866 - val_loss: 2.9981 - val_accuracy: 0.3147\n",
      "Epoch 158/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 2.9726 - accuracy: 0.2874 - val_loss: 2.9943 - val_accuracy: 0.3147\n",
      "Epoch 159/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9712 - accuracy: 0.2803 - val_loss: 2.9904 - val_accuracy: 0.3147\n",
      "Epoch 160/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9697 - accuracy: 0.2812 - val_loss: 2.9866 - val_accuracy: 0.3147\n",
      "Epoch 161/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9582 - accuracy: 0.2823 - val_loss: 2.9828 - val_accuracy: 0.3147\n",
      "Epoch 162/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9520 - accuracy: 0.2885 - val_loss: 2.9791 - val_accuracy: 0.3147\n",
      "Epoch 163/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9428 - accuracy: 0.2833 - val_loss: 2.9753 - val_accuracy: 0.3147\n",
      "Epoch 164/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9726 - accuracy: 0.2710 - val_loss: 2.9716 - val_accuracy: 0.3147\n",
      "Epoch 165/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9465 - accuracy: 0.2817 - val_loss: 2.9678 - val_accuracy: 0.3147\n",
      "Epoch 166/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9231 - accuracy: 0.2952 - val_loss: 2.9641 - val_accuracy: 0.3147\n",
      "Epoch 167/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9414 - accuracy: 0.2828 - val_loss: 2.9604 - val_accuracy: 0.3147\n",
      "Epoch 168/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9358 - accuracy: 0.2852 - val_loss: 2.9567 - val_accuracy: 0.3147\n",
      "Epoch 169/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9299 - accuracy: 0.2839 - val_loss: 2.9531 - val_accuracy: 0.3128\n",
      "Epoch 170/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9312 - accuracy: 0.2926 - val_loss: 2.9494 - val_accuracy: 0.3128\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9250 - accuracy: 0.2880 - val_loss: 2.9458 - val_accuracy: 0.3147\n",
      "Epoch 172/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9119 - accuracy: 0.2881 - val_loss: 2.9422 - val_accuracy: 0.3147\n",
      "Epoch 173/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9294 - accuracy: 0.2910 - val_loss: 2.9386 - val_accuracy: 0.3147\n",
      "Epoch 174/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9383 - accuracy: 0.2827 - val_loss: 2.9350 - val_accuracy: 0.3147\n",
      "Epoch 175/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9312 - accuracy: 0.2794 - val_loss: 2.9314 - val_accuracy: 0.3147\n",
      "Epoch 176/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9206 - accuracy: 0.2794 - val_loss: 2.9278 - val_accuracy: 0.3147\n",
      "Epoch 177/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9087 - accuracy: 0.2917 - val_loss: 2.9243 - val_accuracy: 0.3147\n",
      "Epoch 178/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9268 - accuracy: 0.2784 - val_loss: 2.9208 - val_accuracy: 0.3147\n",
      "Epoch 179/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9190 - accuracy: 0.2793 - val_loss: 2.9173 - val_accuracy: 0.3147\n",
      "Epoch 180/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.9137 - accuracy: 0.2827 - val_loss: 2.9137 - val_accuracy: 0.3147\n",
      "Epoch 181/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8898 - accuracy: 0.2896 - val_loss: 2.9102 - val_accuracy: 0.3147\n",
      "Epoch 182/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8968 - accuracy: 0.2931 - val_loss: 2.9067 - val_accuracy: 0.3128\n",
      "Epoch 183/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8926 - accuracy: 0.2819 - val_loss: 2.9033 - val_accuracy: 0.3128\n",
      "Epoch 184/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8871 - accuracy: 0.2916 - val_loss: 2.8998 - val_accuracy: 0.3166\n",
      "Epoch 185/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8964 - accuracy: 0.2841 - val_loss: 2.8964 - val_accuracy: 0.3147\n",
      "Epoch 186/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8960 - accuracy: 0.2832 - val_loss: 2.8929 - val_accuracy: 0.3147\n",
      "Epoch 187/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8859 - accuracy: 0.2888 - val_loss: 2.8895 - val_accuracy: 0.3147\n",
      "Epoch 188/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8908 - accuracy: 0.2780 - val_loss: 2.8861 - val_accuracy: 0.3147\n",
      "Epoch 189/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8451 - accuracy: 0.2998 - val_loss: 2.8827 - val_accuracy: 0.3147\n",
      "Epoch 190/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8993 - accuracy: 0.2875 - val_loss: 2.8793 - val_accuracy: 0.3128\n",
      "Epoch 191/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8515 - accuracy: 0.2951 - val_loss: 2.8760 - val_accuracy: 0.3128\n",
      "Epoch 192/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8509 - accuracy: 0.2924 - val_loss: 2.8727 - val_accuracy: 0.3128\n",
      "Epoch 193/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8458 - accuracy: 0.2963 - val_loss: 2.8693 - val_accuracy: 0.3128\n",
      "Epoch 194/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8796 - accuracy: 0.2878 - val_loss: 2.8660 - val_accuracy: 0.3128\n",
      "Epoch 195/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8382 - accuracy: 0.2902 - val_loss: 2.8626 - val_accuracy: 0.3128\n",
      "Epoch 196/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8528 - accuracy: 0.2987 - val_loss: 2.8593 - val_accuracy: 0.3128\n",
      "Epoch 197/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8506 - accuracy: 0.2942 - val_loss: 2.8561 - val_accuracy: 0.3128\n",
      "Epoch 198/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8571 - accuracy: 0.2856 - val_loss: 2.8528 - val_accuracy: 0.3147\n",
      "Epoch 199/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8406 - accuracy: 0.2907 - val_loss: 2.8495 - val_accuracy: 0.3147\n",
      "Epoch 200/200\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8006 - accuracy: 0.3062 - val_loss: 2.8463 - val_accuracy: 0.3147\n",
      "11/11 [==============================] - 0s 835us/step - loss: 2.8463 - accuracy: 0.3147\n",
      "[INFO] accuracy: 31.47%\n",
      "[INFO] Loss: 2.84627628326416\n",
      "Time:  0:00:31.960475\n"
     ]
    }
   ],
   "source": [
    "#This is the best model we found. For additional models, check out I_notebook.ipynb\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPool2D \n",
    "from tensorflow.keras.layers import Convolution2D,MaxPooling2D,Dropout,Dense,Flatten,BatchNormalization,Conv2D\n",
    "start = datetime.datetime.now()\n",
    "#print(train_data[2].shape[1])\n",
    "# model = Sequential() \n",
    "# #model.add(Flatten(input_shape=train_data.shape[1:])) \n",
    "# model.add(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.5)) \n",
    "# model.add(Dense(50, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.3)) \n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#    optimizer=optimizers.Adam(lr=1e-4),\n",
    "#    metrics=['acc'])\n",
    "\n",
    "# model.add(Dense(200, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.5)) \n",
    "# model.add(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(50, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#    optimizer=optimizers.Adam(lr=1e-4),\n",
    "# #    metrics=['acc'])\n",
    "\n",
    "###########\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "# model = Sequential() \n",
    "# model.add(BatchNormalization()) \n",
    "# model.add(Dropout(0.7)) \n",
    "# model.add(Dense(512)) \n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation(swish_act))\n",
    "\n",
    "# model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#    optimizer=optimizers.Adam(lr=1e-4),\n",
    "#    metrics=['acc'])\n",
    "######################################################################\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(tensorflow.keras.layers.Dense(1024, activation='relu'))\n",
    "# model.add(tensorflow.keras.layers.Dense(1024, activation='relu'))\n",
    "# model.add(tensorflow.keras.layers.Dense(512, activation='relu'))\n",
    "\n",
    "#model.add(BatchNormalization()) \n",
    "#model.add(Dropout(0.7)) \n",
    "#model.add(Dense(512)) \n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation(swish_act))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(128)) \n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation(swish_act))\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "   optimizer=optimizers.Adam(lr=1e-4),\n",
    "   metrics=['acc'])\n",
    "\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# # building 2 fully connected layer \n",
    "# #x = model.output\n",
    "\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Dropout(0.7)(x)\n",
    "\n",
    "# x = Dense(512)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation(swish_act)(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "\n",
    "# x = Dense(128)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation(swish_act)(x)\n",
    "\n",
    "# output layer\n",
    "#predictions = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "#model_final = Model(inputs = model.input, outputs = predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(0.0001),\n",
    "              metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, verbose=1,)\n",
    "mcp_save = ModelCheckpoint('EnetB0_CIFAR10_TL.h5', save_best_only=True, monitor='val_acc')\n",
    "\n",
    "history = model.fit(train_data, train_labels, \n",
    "   epochs=200,\n",
    "   batch_size=batch_size, \n",
    "   validation_data=(validation_data, validation_labels))\n",
    "\n",
    "model.save_weights(top_model_weights_path)\n",
    "(eval_loss, eval_accuracy) = model.evaluate( \n",
    "    validation_data, validation_labels, batch_size=batch_size,     verbose=1)\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
    "print('[INFO] Loss: {}'.format(eval_loss)) \n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 55)                55055     \n",
      "=================================================================\n",
      "Total params: 55,055\n",
      "Trainable params: 55,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8331 - accuracy: 0.2934 - val_loss: 2.8430 - val_accuracy: 0.3147\n",
      "Epoch 2/10\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8301 - accuracy: 0.2932 - val_loss: 2.8398 - val_accuracy: 0.3147\n",
      "Epoch 3/10\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8271 - accuracy: 0.2940 - val_loss: 2.8366 - val_accuracy: 0.3147\n",
      "Epoch 4/10\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8242 - accuracy: 0.2938 - val_loss: 2.8334 - val_accuracy: 0.3147\n",
      "Epoch 5/10\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8213 - accuracy: 0.2942 - val_loss: 2.8302 - val_accuracy: 0.3147\n",
      "Epoch 6/10\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8184 - accuracy: 0.2946 - val_loss: 2.8270 - val_accuracy: 0.3166\n",
      "Epoch 7/10\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 2.8155 - accuracy: 0.2953 - val_loss: 2.8239 - val_accuracy: 0.3166\n",
      "Epoch 8/10\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 2.8126 - accuracy: 0.2961 - val_loss: 2.8207 - val_accuracy: 0.3166\n",
      "Epoch 9/10\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8097 - accuracy: 0.2963 - val_loss: 2.8175 - val_accuracy: 0.3166\n",
      "Epoch 10/10\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8068 - accuracy: 0.2967 - val_loss: 2.8145 - val_accuracy: 0.3166\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 2.8145 - accuracy: 0.3166\n",
      "[INFO] accuracy: 31.66%\n",
      "[INFO] Loss: 2.8145031929016113\n",
      "Time:  0:00:33.724602\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(train_data, train_labels, \n",
    "   epochs=10,\n",
    "   batch_size=batch_size, \n",
    "   validation_data=(validation_data, validation_labels))\n",
    "\n",
    "model.save_weights(top_model_weights_path)\n",
    "(eval_loss, eval_accuracy) = model.evaluate( \n",
    "    validation_data, validation_labels, batch_size=batch_size,     verbose=1)\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
    "print('[INFO] Loss: {}'.format(eval_loss)) \n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# y_predict=model.predict(validation_data)\n",
    "# y_predict= np.argmax(y_predict, axis=1)\n",
    "# #print(y_predict)\n",
    "# y_true=np.argmax(validation_labels, axis=1)\n",
    "# #print(y_true)\n",
    "# print(classification_report(y_predict,y_true))\n",
    "# y_predict=list(y_predict)\n",
    "# y_true=list(y_true)\n",
    "# with open(\"Mobilenet_DataAug.txt\",\"w+\") as f:\n",
    "#     f.write(str([y_predict,y_true]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c66604de1aec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print(y_true[x], y_predict[x])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "indexes=[]\n",
    "for x in range(len(y_true)):\n",
    "    if y_true[x]!=y_predict[x]:\n",
    "    #print(y_true[x], y_predict[x])\n",
    "        indexes.append(x)\n",
    "print(len(indexes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# dir_hapus=\"/media/hades/DISK 0 - Drive 3/Fish/Test7/Test/\"\n",
    "# for i in indexes[0:-20]:\n",
    "#     print(list_file[i])\n",
    "#     shutil.move(dir_hapus+list_file[i],\"/media/hades/DISK 0 - Drive 3/Fish/Test7/Test_Backup/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphing our training and validation\n",
    "#print(history.history)\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.ylabel('accuracy') \n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.ylabel('loss') \n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil=os.listdir(\"/media/hades/DISK 0 - Drive 3/Fish/Test7/Test/\")\n",
    "\n",
    "consumable=pd.read_csv(\"Commercial.txt\",delimiter=r\"\\t+\")\n",
    "consumable2=pd.read_csv(\"Game_Fishes.txt\",delimiter=r\"\\t+\")\n",
    "\n",
    "unconsumable=pd.read_csv(\"Dangerous.txt\",delimiter=r\"\\t+\")\n",
    "\n",
    "\n",
    "unconsumable_final=[]\n",
    "consumable_final=[]\n",
    "consumable2_final=[]\n",
    "\n",
    "for x in consumable['Species']:\n",
    "    x=x.replace(' ','_')\n",
    "    consumable_final.append(x)\n",
    "for x in unconsumable['Species']:\n",
    "    x=x.replace(' ','_')\n",
    "    unconsumable_final.append(x)\n",
    "for x in consumable2['Species']:\n",
    "    x=x.replace(' ','_')\n",
    "    consumable2_final.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sort_Tuple(tup):  \n",
    "  \n",
    "    # reverse = None (Sorts in Ascending order)  \n",
    "    # key is set to sort using second element of  \n",
    "    # sublist lambda has been used  \n",
    "    tup.sort(key = lambda x: x[2],reverse=True)  \n",
    "    return tup  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_image(file_path):\n",
    "   #print('[INFO] loading and preprocessing image…') \n",
    "   image = load_img(file_path, target_size=(224, 224)) \n",
    "   image = img_to_array(image) \n",
    "   image = np.expand_dims(image, axis=0)\n",
    "   image /= 255. \n",
    "   return image\n",
    "def test_single_image(path):\n",
    "  animals = os.listdir('/media/hades/DISK 0 - Drive 3/Fish/Test7/Train')\n",
    "  animals=sorted(animals)\n",
    "  images = read_image(path)\n",
    "  time.sleep(.5)\n",
    "  bt_prediction = vgg16.predict(images) \n",
    "  preds = model.predict_proba(bt_prediction)\n",
    "  #print(\"preds:\",preds)\n",
    "  finalpercent=Sort_Tuple(list(zip(range(0,667), animals , preds[0])))\n",
    "  #print(finalpercent)\n",
    "  #for idx, animal, x in zip(range(0,10), animals , preds[0]):\n",
    "   #print('ID: {}, Label: {} {}%'.format(idx, animal, round(x*100,2) ))\n",
    "    \n",
    "  #print('Final Decision:')\n",
    "  time.sleep(.5)\n",
    "  for x in range(3):\n",
    "   #print('.'*(x+1))\n",
    "   time.sleep(.2)\n",
    "  class_predicted = model.predict_classes(bt_prediction)\n",
    "  class_dictionary = generator_top.class_indices \n",
    "  inv_map = {v: k for k, v in class_dictionary.items()} \n",
    "  #print(\"PRED:\",class_dictionary.items())\n",
    "  finalpred='ID: {}, Label: {}'.format(class_predicted[0],  inv_map[class_predicted[0]])\n",
    "  finalpred2=[]\n",
    "  for x,y in enumerate(finalpercent[:5]):\n",
    "        finalpred2.append('ID: {}, Label: {} {}%'.format(y[0],y[1],y[2]))\n",
    "  #print(finalpred2)\n",
    "  #print('ID: {}, Label: {}'.format(class_predicted[0],  inv_map[class_predicted[0]])) \n",
    "  return finalpred,finalpred2,finalpercent\n",
    "\n",
    "path = '/media/hades/DISK 0 - Drive 3/Fish/Test7/Test/Naso/Naso_unicornis_0060.jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(species):\n",
    "    nama_species=species[0][1].replace(\"_\",' ')\n",
    "    print(\"namaSP:\",species[0][1])\n",
    "    pd.set_option('display.max_rows', unconsumable.shape[0]+1)\n",
    "    print(unconsumable)\n",
    "    if species[0][1] in consumable_final:\n",
    "        row=consumable.loc[consumable['Species'] == nama_species]\n",
    "        row_list=[]\n",
    "        \n",
    "        print(\"Tes1:\",row['Species'])\n",
    "        row_list.append(\"CONSUMABLE\")\n",
    "        row_list.append(\"Species Name: %s\" %row['Species'].values[0])\n",
    "        row_list.append(\"Order: %s\" %row['Order'].values[0])\n",
    "        row_list.append(\"Family: %s\" %row['Family'].values[0])\n",
    "        row_list.append(\"Foreign Name: %s\" %row['FishBase name'].values[0])\n",
    "        row_list.append(\"Local Name: %s\" %row['Name'].values[0])\n",
    "        row_list.append(\"Description: %s\" %row['Use'].values[0])\n",
    "    elif species[0][1] in consumable2_final:\n",
    "        row=consumable2.loc[consumable2['Species'] == nama_species]\n",
    "        row_list=[]\n",
    "        \n",
    "        print(\"Tes2:\",row['Species'])\n",
    "        row_list.append(\"CONSUMABLE\")\n",
    "        row_list.append(\"Species Name: %s\" %row['Species'].values[0])\n",
    "        row_list.append(\"Order: %s\" %row['Order'].values[0])\n",
    "        row_list.append(\"Family: %s\" %row['Family'].values[0])\n",
    "        row_list.append(\"Foreign Name: %s\" %row['FishBase name'].values[0])\n",
    "        row_list.append(\"Local Name: %s\" %row['Name'].values[0])\n",
    "        row_list.append(\"Description: %s\" %row['No'].values[0])\n",
    "    else:\n",
    "        print(\"ASDDDDDDDDDDDDDDDD\",nama_species)\n",
    "        row=unconsumable.loc[unconsumable['Species'] == nama_species]\n",
    "        row_list=[]\n",
    "        \n",
    "        print(\"Tes3:\",row['Species'])\n",
    "        row_list.append(\"UNCONSUMABLE\")\n",
    "        row_list.append(\"Species Name: %s\" %row['Species'].values[0])\n",
    "        row_list.append(\"Order: %s\" %row['Order'].values[0])\n",
    "        row_list.append(\"Family: %s\" %row['Family'].values[0])\n",
    "        row_list.append(\"Foreign Name: %s\" %row['FishBase name'].values[0])\n",
    "        row_list.append(\"Local Name: %s\" %row['Name'].values[0])\n",
    "        row_list.append(\"Description: %s\" %row['Danger'].values[0])\n",
    "        \n",
    "    return row_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "%matplotlib tk\n",
    "\n",
    "img = mpimg.imread(path)\n",
    "plt.imshow(img)\n",
    "a,b,c=test_single_image(path)\n",
    "print(\"HH\",b)\n",
    "#print(get_data(c))\n",
    "textstr = '\\n'.join((\n",
    "    r'Classification Result',\n",
    "        r'',\n",
    "    r'',\n",
    "    r'Final Result:',\n",
    "        r'',\n",
    "        r'%s'%a,\n",
    "        r'',\n",
    "        r'DEBUG: ',\n",
    "        r'',\n",
    "        r'%s'%b[0],\n",
    "        r'%s'%b[1],\n",
    "        r'%s'%b[2],\n",
    "        r'%s'%b[3],\n",
    "        r'%s'%b[4]\n",
    "    ))\n",
    "d=get_data(c)\n",
    "#print(get_data(c))\n",
    "#print(get_data(c))\n",
    "textstr2 = '\\n'.join((\n",
    "        r'%s'%d[0],\n",
    "        r'Details',\n",
    "        r'',\n",
    "        r'%s'%d[1],\n",
    "        r'%s'%d[2],\n",
    "        r'%s'%d[3],\n",
    "        r'%s'%d[4],\n",
    "        r'%s'%d[5],\n",
    "        r'%s'%d[6],\n",
    "        \n",
    "    ))\n",
    "plt.xlabel(' ')\n",
    "plt.ylabel(' ')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "plt.subplots_adjust(left=0.35)\n",
    "\n",
    "plt.text(0.05, 0.95, textstr, transform=plt.gcf().transFigure, fontsize=10,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.text(0.05, 0.42, textstr2, transform=plt.gcf().transFigure, fontsize=10,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
