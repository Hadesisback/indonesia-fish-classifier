{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## import pandas as pd\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import numpy as np \n",
    "import itertools\n",
    "import tensorflow.keras\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPool2D \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization, Flatten\n",
    "from tensorflow.keras import applications \n",
    "from keras.utils.np_utils import to_categorical \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "import efficientnet.keras as enet\n",
    "import keras_applications as app\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Model\n",
    "import keras\n",
    "#%matplotlib inline\n",
    "import math \n",
    "import datetime\n",
    "import time\n",
    "CUDA_VISIBLE_DEVICES = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Default dimensions we found online\n",
    "img_width, img_height = 224, 224 \n",
    " \n",
    "#Create a bottleneck file\n",
    "top_model_weights_path = \"bottleneck_fc_model_50_50.h5\"\n",
    "# loading up our datasets\n",
    "train_data_dir = '/media/hades/DISK 1 - Drive 2/Fish/Test7/Train' \n",
    "validation_data_dir = '/media/hades/DISK 1 - Drive 2/Fish/Test7/Validate' \n",
    "test_data_dir = '/media/hades/DISK 1 - Drive 2/Fish/Test7/Test'\n",
    " \n",
    "# number of epochs to train top model \n",
    "epochs = 50 #this has been changed after multiple model run \n",
    "# batch size used by flow_from_directory and predict_generator \n",
    "batch_size = 50 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAS\n",
      "HADOH\n"
     ]
    }
   ],
   "source": [
    "import efficientnet.keras as enet\n",
    "\n",
    "#Loading vgc16 model\n",
    "print(\"HAS\")\n",
    "#vgg16 = app.efficientnet.EfficientNetB7(utils = tensorflow.keras.utils, models=tensorflow.keras.models, layers = tensorflow.keras.layers, backend = tensorflow.keras.backend, include_top=False, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "#vgg16 = app.vgg16.VGG16(utils = keras.utils, models=keras.models, layers = keras.layers, backend = keras.backend, include_top=False, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "#vgg16 = applications.VGG16(include_top=False, weights=’imagenet’)\n",
    "#vgg16 = app.mobilenet_v3.MobileNetV3(utils = keras.utils, models=keras.models, layers = keras.layers, backend = keras.backend, include_top=False, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "#vgg16 = app.resnet50.ResNet50(utils = keras.utils, models=keras.models, layers = keras.layers, backend = keras.backend, include_top=False, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "vgg16=app.mobilenet.MobileNet(utils = tensorflow.keras.utils, models=tensorflow.keras.models, layers = tensorflow.keras.layers, backend = tensorflow.keras.backend, include_top=True, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "# datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "#       )\n",
    "print(\"HADOH\")\n",
    "#needed to create the bottleneck .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenet_1.00_224\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 225, 225, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_preds (Conv2D)          (None, 1, 1, 1000)        1025000   \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "act_softmax (Activation)     (None, 1000)              0         \n",
      "=================================================================\n",
      "Total params: 4,253,864\n",
      "Trainable params: 4,231,976\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "\n",
    "#plot_model(model, to_file='toplayers.png', show_shapes=True, show_layer_names=True)\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASD1\n",
      "Found 5150 images belonging to 55 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hades/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n",
      "/home/hades/.local/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0:01:50.886410\n"
     ]
    }
   ],
   "source": [
    "#__this can take an hour and half to run so only run it once. \n",
    "#once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__\n",
    "start = datetime.datetime.now()\n",
    "print(\"ASD1\")\n",
    "generator = datagen.flow_from_directory( \n",
    "    train_data_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode=None, \n",
    "    shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator.filenames) \n",
    "num_classes = len(generator.class_indices) \n",
    " \n",
    "predict_size_train = int(math.ceil(nb_train_samples / batch_size)) \n",
    " \n",
    "bottleneck_features_train = vgg16.predict_generator(generator, predict_size_train) \n",
    "vgg16.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "np.save('bottleneck_features_train_50_50.npy', bottleneck_features_train)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 537 images belonging to 55 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hades/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0:00:13.482888\n"
     ]
    }
   ],
   "source": [
    "#__this can take an hour and half to run so only run it once. \n",
    "#once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__\n",
    "start = datetime.datetime.now()\n",
    " \n",
    "generator = datagen.flow_from_directory( \n",
    "    test_data_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode=None, \n",
    "    shuffle=False) \n",
    " \n",
    "nb_validation_samples = len(generator.filenames) \n",
    "num_classes = len(generator.class_indices) \n",
    " \n",
    "predict_size_validation = int(math.ceil(nb_validation_samples / batch_size)) \n",
    " \n",
    "bottleneck_features_validation = vgg16.predict_generator(generator, predict_size_validation) \n",
    " \n",
    "np.save('bottleneck_features_validation_50_50.npy', bottleneck_features_validation)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5150 images belonging to 55 classes.\n"
     ]
    }
   ],
   "source": [
    "#training data\n",
    "generator_top = datagen.flow_from_directory( \n",
    "   train_data_dir, \n",
    "   target_size=(img_width, img_height), \n",
    "   batch_size=batch_size, \n",
    "   class_mode='categorical', \n",
    "   shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator_top.filenames) \n",
    "num_classes = len(generator_top.class_indices) \n",
    " \n",
    "# load the bottleneck features saved earlier \n",
    "train_data = np.load('bottleneck_features_train_50_50.npy') \n",
    " \n",
    "# get the class labels for the training data, in the original order \n",
    "train_labels = generator_top.classes \n",
    " \n",
    "# convert the training labels to categorical vectors \n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 537 images belonging to 55 classes.\n"
     ]
    }
   ],
   "source": [
    "#training data\n",
    "generator_top = datagen.flow_from_directory( \n",
    "   test_data_dir, \n",
    "   target_size=(img_width, img_height), \n",
    "   batch_size=batch_size, \n",
    "   class_mode='categorical', \n",
    "   shuffle=False) \n",
    " \n",
    "nb_validation_samples = len(generator_top.filenames) \n",
    "num_classes = len(generator_top.class_indices) \n",
    " \n",
    "# load the bottleneck features saved earlier \n",
    "validation_data = np.load('bottleneck_features_validation_50_50.npy') \n",
    " \n",
    "# get the class labels for the training data, in the original order \n",
    "validation_labels = generator_top.classes \n",
    " \n",
    "# convert the training labels to categorical vectors \n",
    "validation_labels = to_categorical(validation_labels, num_classes=num_classes)\n",
    "\n",
    "#validation_data = validation_data.reshape((-1, 224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_file=generator_top.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swish defination\n",
    "from tensorflow.keras.backend import sigmoid\n",
    "\n",
    "class SwishActivation(Activation):\n",
    "    \n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(SwishActivation, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'swish_act'\n",
    "\n",
    "def swish_act(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation\n",
    "get_custom_objects().update({'swish_act': SwishActivation(swish_act)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "103/103 [==============================] - 1s 4ms/step - loss: 4.0069 - accuracy: 0.0206 - val_loss: 3.9994 - val_accuracy: 0.0559\n",
      "Epoch 2/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.9954 - accuracy: 0.0823 - val_loss: 3.9885 - val_accuracy: 0.0782\n",
      "Epoch 3/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.9841 - accuracy: 0.0966 - val_loss: 3.9778 - val_accuracy: 0.0950\n",
      "Epoch 4/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.9730 - accuracy: 0.0986 - val_loss: 3.9672 - val_accuracy: 0.0968\n",
      "Epoch 5/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.9619 - accuracy: 0.0991 - val_loss: 3.9566 - val_accuracy: 0.1043\n",
      "Epoch 6/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.9490 - accuracy: 0.1114 - val_loss: 3.9462 - val_accuracy: 0.1080\n",
      "Epoch 7/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.9384 - accuracy: 0.1147 - val_loss: 3.9359 - val_accuracy: 0.1099\n",
      "Epoch 8/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.9270 - accuracy: 0.1231 - val_loss: 3.9256 - val_accuracy: 0.1099\n",
      "Epoch 9/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.9177 - accuracy: 0.1236 - val_loss: 3.9155 - val_accuracy: 0.1099\n",
      "Epoch 10/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.9064 - accuracy: 0.1160 - val_loss: 3.9054 - val_accuracy: 0.1173\n",
      "Epoch 11/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.8964 - accuracy: 0.1287 - val_loss: 3.8954 - val_accuracy: 0.1192\n",
      "Epoch 12/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.8854 - accuracy: 0.1419 - val_loss: 3.8855 - val_accuracy: 0.1285\n",
      "Epoch 13/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.8774 - accuracy: 0.1489 - val_loss: 3.8757 - val_accuracy: 0.1359\n",
      "Epoch 14/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.8651 - accuracy: 0.1661 - val_loss: 3.8660 - val_accuracy: 0.1527\n",
      "Epoch 15/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.8590 - accuracy: 0.1639 - val_loss: 3.8564 - val_accuracy: 0.1620\n",
      "Epoch 16/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.8438 - accuracy: 0.1837 - val_loss: 3.8468 - val_accuracy: 0.1732\n",
      "Epoch 17/200\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 3.8322 - accuracy: 0.1872 - val_loss: 3.8373 - val_accuracy: 0.1713\n",
      "Epoch 18/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.8223 - accuracy: 0.1937 - val_loss: 3.8280 - val_accuracy: 0.1806\n",
      "Epoch 19/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.8087 - accuracy: 0.2027 - val_loss: 3.8186 - val_accuracy: 0.1806\n",
      "Epoch 20/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.7986 - accuracy: 0.2030 - val_loss: 3.8094 - val_accuracy: 0.1825\n",
      "Epoch 21/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.7907 - accuracy: 0.2012 - val_loss: 3.8003 - val_accuracy: 0.1825\n",
      "Epoch 22/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.7844 - accuracy: 0.2024 - val_loss: 3.7912 - val_accuracy: 0.1881\n",
      "Epoch 23/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.7744 - accuracy: 0.2031 - val_loss: 3.7822 - val_accuracy: 0.1899\n",
      "Epoch 24/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.7605 - accuracy: 0.2206 - val_loss: 3.7732 - val_accuracy: 0.1937\n",
      "Epoch 25/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.7571 - accuracy: 0.2130 - val_loss: 3.7644 - val_accuracy: 0.1974\n",
      "Epoch 26/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.7438 - accuracy: 0.2160 - val_loss: 3.7556 - val_accuracy: 0.2086\n",
      "Epoch 27/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.7353 - accuracy: 0.2316 - val_loss: 3.7469 - val_accuracy: 0.2104\n",
      "Epoch 28/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.7244 - accuracy: 0.2364 - val_loss: 3.7382 - val_accuracy: 0.2197\n",
      "Epoch 29/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.7146 - accuracy: 0.2409 - val_loss: 3.7296 - val_accuracy: 0.2384\n",
      "Epoch 30/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.7096 - accuracy: 0.2350 - val_loss: 3.7211 - val_accuracy: 0.2402\n",
      "Epoch 31/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.6963 - accuracy: 0.2482 - val_loss: 3.7127 - val_accuracy: 0.2402\n",
      "Epoch 32/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.6878 - accuracy: 0.2411 - val_loss: 3.7043 - val_accuracy: 0.2402\n",
      "Epoch 33/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.6854 - accuracy: 0.2301 - val_loss: 3.6960 - val_accuracy: 0.2421\n",
      "Epoch 34/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.6706 - accuracy: 0.2452 - val_loss: 3.6877 - val_accuracy: 0.2439\n",
      "Epoch 35/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.6586 - accuracy: 0.2462 - val_loss: 3.6795 - val_accuracy: 0.2458\n",
      "Epoch 36/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.6603 - accuracy: 0.2419 - val_loss: 3.6714 - val_accuracy: 0.2458\n",
      "Epoch 37/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.6512 - accuracy: 0.2375 - val_loss: 3.6634 - val_accuracy: 0.2458\n",
      "Epoch 38/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.6417 - accuracy: 0.2350 - val_loss: 3.6554 - val_accuracy: 0.2458\n",
      "Epoch 39/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.6292 - accuracy: 0.2469 - val_loss: 3.6475 - val_accuracy: 0.2458\n",
      "Epoch 40/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.6253 - accuracy: 0.2397 - val_loss: 3.6397 - val_accuracy: 0.2439\n",
      "Epoch 41/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.6121 - accuracy: 0.2382 - val_loss: 3.6319 - val_accuracy: 0.2439\n",
      "Epoch 42/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.5971 - accuracy: 0.2427 - val_loss: 3.6242 - val_accuracy: 0.2439\n",
      "Epoch 43/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.5973 - accuracy: 0.2409 - val_loss: 3.6165 - val_accuracy: 0.2439\n",
      "Epoch 44/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.5912 - accuracy: 0.2344 - val_loss: 3.6089 - val_accuracy: 0.2421\n",
      "Epoch 45/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.5708 - accuracy: 0.2518 - val_loss: 3.6014 - val_accuracy: 0.2421\n",
      "Epoch 46/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.5793 - accuracy: 0.2419 - val_loss: 3.5939 - val_accuracy: 0.2421\n",
      "Epoch 47/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.5630 - accuracy: 0.2414 - val_loss: 3.5865 - val_accuracy: 0.2421\n",
      "Epoch 48/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.5531 - accuracy: 0.2499 - val_loss: 3.5792 - val_accuracy: 0.2439\n",
      "Epoch 49/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.5407 - accuracy: 0.2482 - val_loss: 3.5719 - val_accuracy: 0.2439\n",
      "Epoch 50/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.5372 - accuracy: 0.2471 - val_loss: 3.5646 - val_accuracy: 0.2439\n",
      "Epoch 51/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.5297 - accuracy: 0.2531 - val_loss: 3.5575 - val_accuracy: 0.2421\n",
      "Epoch 52/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.5259 - accuracy: 0.2520 - val_loss: 3.5503 - val_accuracy: 0.2402\n",
      "Epoch 53/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.5175 - accuracy: 0.2388 - val_loss: 3.5433 - val_accuracy: 0.2421\n",
      "Epoch 54/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.5087 - accuracy: 0.2432 - val_loss: 3.5363 - val_accuracy: 0.2421\n",
      "Epoch 55/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.5149 - accuracy: 0.2401 - val_loss: 3.5293 - val_accuracy: 0.2421\n",
      "Epoch 56/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.4973 - accuracy: 0.2442 - val_loss: 3.5225 - val_accuracy: 0.2439\n",
      "Epoch 57/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4732 - accuracy: 0.2540 - val_loss: 3.5156 - val_accuracy: 0.2439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4803 - accuracy: 0.2412 - val_loss: 3.5088 - val_accuracy: 0.2439\n",
      "Epoch 59/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4818 - accuracy: 0.2442 - val_loss: 3.5021 - val_accuracy: 0.2439\n",
      "Epoch 60/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4695 - accuracy: 0.2496 - val_loss: 3.4954 - val_accuracy: 0.2439\n",
      "Epoch 61/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4652 - accuracy: 0.2519 - val_loss: 3.4887 - val_accuracy: 0.2458\n",
      "Epoch 62/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.4571 - accuracy: 0.2430 - val_loss: 3.4822 - val_accuracy: 0.2458\n",
      "Epoch 63/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4512 - accuracy: 0.2372 - val_loss: 3.4756 - val_accuracy: 0.2458\n",
      "Epoch 64/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4430 - accuracy: 0.2460 - val_loss: 3.4692 - val_accuracy: 0.2458\n",
      "Epoch 65/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4275 - accuracy: 0.2479 - val_loss: 3.4627 - val_accuracy: 0.2458\n",
      "Epoch 66/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4096 - accuracy: 0.2563 - val_loss: 3.4563 - val_accuracy: 0.2477\n",
      "Epoch 67/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4167 - accuracy: 0.2503 - val_loss: 3.4500 - val_accuracy: 0.2439\n",
      "Epoch 68/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.4089 - accuracy: 0.2538 - val_loss: 3.4437 - val_accuracy: 0.2458\n",
      "Epoch 69/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.3986 - accuracy: 0.2527 - val_loss: 3.4374 - val_accuracy: 0.2458\n",
      "Epoch 70/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.4040 - accuracy: 0.2491 - val_loss: 3.4313 - val_accuracy: 0.2458\n",
      "Epoch 71/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3905 - accuracy: 0.2478 - val_loss: 3.4251 - val_accuracy: 0.2458\n",
      "Epoch 72/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3836 - accuracy: 0.2544 - val_loss: 3.4189 - val_accuracy: 0.2458\n",
      "Epoch 73/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3760 - accuracy: 0.2491 - val_loss: 3.4129 - val_accuracy: 0.2458\n",
      "Epoch 74/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3756 - accuracy: 0.2499 - val_loss: 3.4069 - val_accuracy: 0.2458\n",
      "Epoch 75/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3630 - accuracy: 0.2523 - val_loss: 3.4009 - val_accuracy: 0.2439\n",
      "Epoch 76/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3513 - accuracy: 0.2483 - val_loss: 3.3950 - val_accuracy: 0.2439\n",
      "Epoch 77/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3595 - accuracy: 0.2545 - val_loss: 3.3891 - val_accuracy: 0.2439\n",
      "Epoch 78/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3471 - accuracy: 0.2471 - val_loss: 3.3832 - val_accuracy: 0.2458\n",
      "Epoch 79/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3414 - accuracy: 0.2482 - val_loss: 3.3775 - val_accuracy: 0.2439\n",
      "Epoch 80/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3312 - accuracy: 0.2487 - val_loss: 3.3717 - val_accuracy: 0.2421\n",
      "Epoch 81/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3297 - accuracy: 0.2516 - val_loss: 3.3660 - val_accuracy: 0.2421\n",
      "Epoch 82/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3211 - accuracy: 0.2579 - val_loss: 3.3604 - val_accuracy: 0.2458\n",
      "Epoch 83/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3133 - accuracy: 0.2621 - val_loss: 3.3548 - val_accuracy: 0.2458\n",
      "Epoch 84/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3125 - accuracy: 0.2567 - val_loss: 3.3491 - val_accuracy: 0.2477\n",
      "Epoch 85/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3122 - accuracy: 0.2550 - val_loss: 3.3436 - val_accuracy: 0.2477\n",
      "Epoch 86/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2946 - accuracy: 0.2590 - val_loss: 3.3381 - val_accuracy: 0.2495\n",
      "Epoch 87/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.3061 - accuracy: 0.2528 - val_loss: 3.3326 - val_accuracy: 0.2495\n",
      "Epoch 88/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2918 - accuracy: 0.2632 - val_loss: 3.3272 - val_accuracy: 0.2514\n",
      "Epoch 89/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2735 - accuracy: 0.2666 - val_loss: 3.3218 - val_accuracy: 0.2533\n",
      "Epoch 90/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2762 - accuracy: 0.2568 - val_loss: 3.3164 - val_accuracy: 0.2533\n",
      "Epoch 91/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2627 - accuracy: 0.2656 - val_loss: 3.3111 - val_accuracy: 0.2533\n",
      "Epoch 92/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2778 - accuracy: 0.2490 - val_loss: 3.3059 - val_accuracy: 0.2551\n",
      "Epoch 93/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.2642 - accuracy: 0.2672 - val_loss: 3.3006 - val_accuracy: 0.2551\n",
      "Epoch 94/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.2602 - accuracy: 0.2470 - val_loss: 3.2953 - val_accuracy: 0.2551\n",
      "Epoch 95/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.2433 - accuracy: 0.2639 - val_loss: 3.2901 - val_accuracy: 0.2570\n",
      "Epoch 96/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2675 - accuracy: 0.2543 - val_loss: 3.2850 - val_accuracy: 0.2570\n",
      "Epoch 97/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2407 - accuracy: 0.2642 - val_loss: 3.2798 - val_accuracy: 0.2570\n",
      "Epoch 98/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2394 - accuracy: 0.2527 - val_loss: 3.2748 - val_accuracy: 0.2588\n",
      "Epoch 99/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.2281 - accuracy: 0.2742 - val_loss: 3.2697 - val_accuracy: 0.2588\n",
      "Epoch 100/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2147 - accuracy: 0.2695 - val_loss: 3.2646 - val_accuracy: 0.2588\n",
      "Epoch 101/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2328 - accuracy: 0.2593 - val_loss: 3.2597 - val_accuracy: 0.2588\n",
      "Epoch 102/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2177 - accuracy: 0.2559 - val_loss: 3.2547 - val_accuracy: 0.2607\n",
      "Epoch 103/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2105 - accuracy: 0.2637 - val_loss: 3.2497 - val_accuracy: 0.2607\n",
      "Epoch 104/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2056 - accuracy: 0.2614 - val_loss: 3.2448 - val_accuracy: 0.2607\n",
      "Epoch 105/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.2249 - accuracy: 0.2491 - val_loss: 3.2399 - val_accuracy: 0.2607\n",
      "Epoch 106/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.1985 - accuracy: 0.2585 - val_loss: 3.2351 - val_accuracy: 0.2607\n",
      "Epoch 107/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.1915 - accuracy: 0.2612 - val_loss: 3.2302 - val_accuracy: 0.2607\n",
      "Epoch 108/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.1694 - accuracy: 0.2631 - val_loss: 3.2255 - val_accuracy: 0.2607\n",
      "Epoch 109/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.1803 - accuracy: 0.2601 - val_loss: 3.2207 - val_accuracy: 0.2607\n",
      "Epoch 110/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.1736 - accuracy: 0.2583 - val_loss: 3.2159 - val_accuracy: 0.2607\n",
      "Epoch 111/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.1631 - accuracy: 0.2599 - val_loss: 3.2113 - val_accuracy: 0.2607\n",
      "Epoch 112/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.1800 - accuracy: 0.2607 - val_loss: 3.2066 - val_accuracy: 0.2607\n",
      "Epoch 113/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.1603 - accuracy: 0.2600 - val_loss: 3.2019 - val_accuracy: 0.2588\n",
      "Epoch 114/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.1485 - accuracy: 0.2735 - val_loss: 3.1973 - val_accuracy: 0.2588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 3.1395 - accuracy: 0.2585 - val_loss: 3.1927 - val_accuracy: 0.2607\n",
      "Epoch 116/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.1649 - accuracy: 0.2626 - val_loss: 3.1881 - val_accuracy: 0.2626\n",
      "Epoch 117/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.1311 - accuracy: 0.2684 - val_loss: 3.1835 - val_accuracy: 0.2626\n",
      "Epoch 118/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.1466 - accuracy: 0.2581 - val_loss: 3.1790 - val_accuracy: 0.2626\n",
      "Epoch 119/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.1363 - accuracy: 0.2673 - val_loss: 3.1745 - val_accuracy: 0.2626\n",
      "Epoch 120/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.1278 - accuracy: 0.2684 - val_loss: 3.1701 - val_accuracy: 0.2626\n",
      "Epoch 121/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.1346 - accuracy: 0.2632 - val_loss: 3.1656 - val_accuracy: 0.2626\n",
      "Epoch 122/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.1249 - accuracy: 0.2577 - val_loss: 3.1611 - val_accuracy: 0.2626\n",
      "Epoch 123/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.1062 - accuracy: 0.2692 - val_loss: 3.1567 - val_accuracy: 0.2626\n",
      "Epoch 124/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.1054 - accuracy: 0.2641 - val_loss: 3.1523 - val_accuracy: 0.2626\n",
      "Epoch 125/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.1080 - accuracy: 0.2596 - val_loss: 3.1480 - val_accuracy: 0.2626\n",
      "Epoch 126/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0873 - accuracy: 0.2795 - val_loss: 3.1437 - val_accuracy: 0.2644\n",
      "Epoch 127/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.0866 - accuracy: 0.2772 - val_loss: 3.1393 - val_accuracy: 0.2663\n",
      "Epoch 128/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0873 - accuracy: 0.2678 - val_loss: 3.1350 - val_accuracy: 0.2663\n",
      "Epoch 129/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0835 - accuracy: 0.2770 - val_loss: 3.1307 - val_accuracy: 0.2682\n",
      "Epoch 130/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0756 - accuracy: 0.2685 - val_loss: 3.1265 - val_accuracy: 0.2682\n",
      "Epoch 131/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0716 - accuracy: 0.2698 - val_loss: 3.1223 - val_accuracy: 0.2682\n",
      "Epoch 132/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0682 - accuracy: 0.2692 - val_loss: 3.1180 - val_accuracy: 0.2682\n",
      "Epoch 133/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0844 - accuracy: 0.2689 - val_loss: 3.1138 - val_accuracy: 0.2682\n",
      "Epoch 134/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0718 - accuracy: 0.2663 - val_loss: 3.1097 - val_accuracy: 0.2682\n",
      "Epoch 135/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0561 - accuracy: 0.2749 - val_loss: 3.1055 - val_accuracy: 0.2682\n",
      "Epoch 136/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0461 - accuracy: 0.2702 - val_loss: 3.1014 - val_accuracy: 0.2682\n",
      "Epoch 137/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0456 - accuracy: 0.2700 - val_loss: 3.0972 - val_accuracy: 0.2682\n",
      "Epoch 138/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.0477 - accuracy: 0.2756 - val_loss: 3.0932 - val_accuracy: 0.2700\n",
      "Epoch 139/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.0370 - accuracy: 0.2792 - val_loss: 3.0890 - val_accuracy: 0.2719\n",
      "Epoch 140/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.0259 - accuracy: 0.2789 - val_loss: 3.0851 - val_accuracy: 0.2700\n",
      "Epoch 141/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.0415 - accuracy: 0.2772 - val_loss: 3.0810 - val_accuracy: 0.2719\n",
      "Epoch 142/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0329 - accuracy: 0.2843 - val_loss: 3.0769 - val_accuracy: 0.2719\n",
      "Epoch 143/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0358 - accuracy: 0.2667 - val_loss: 3.0730 - val_accuracy: 0.2719\n",
      "Epoch 144/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0180 - accuracy: 0.2782 - val_loss: 3.0689 - val_accuracy: 0.2719\n",
      "Epoch 145/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.0266 - accuracy: 0.2724 - val_loss: 3.0650 - val_accuracy: 0.2719\n",
      "Epoch 146/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0308 - accuracy: 0.2627 - val_loss: 3.0610 - val_accuracy: 0.2719\n",
      "Epoch 147/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.0160 - accuracy: 0.2732 - val_loss: 3.0571 - val_accuracy: 0.2756\n",
      "Epoch 148/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 3.0132 - accuracy: 0.2750 - val_loss: 3.0532 - val_accuracy: 0.2756\n",
      "Epoch 149/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.0130 - accuracy: 0.2825 - val_loss: 3.0493 - val_accuracy: 0.2756\n",
      "Epoch 150/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.0230 - accuracy: 0.2691 - val_loss: 3.0454 - val_accuracy: 0.2756\n",
      "Epoch 151/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.0147 - accuracy: 0.2734 - val_loss: 3.0415 - val_accuracy: 0.2793\n",
      "Epoch 152/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 3.0016 - accuracy: 0.2761 - val_loss: 3.0377 - val_accuracy: 0.2812\n",
      "Epoch 153/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 2.9918 - accuracy: 0.2857 - val_loss: 3.0339 - val_accuracy: 0.2812\n",
      "Epoch 154/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 2.9961 - accuracy: 0.2788 - val_loss: 3.0300 - val_accuracy: 0.2831\n",
      "Epoch 155/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 2.9814 - accuracy: 0.2791 - val_loss: 3.0262 - val_accuracy: 0.2868\n",
      "Epoch 156/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 2.9748 - accuracy: 0.2786 - val_loss: 3.0225 - val_accuracy: 0.2868\n",
      "Epoch 157/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 2.9785 - accuracy: 0.2761 - val_loss: 3.0187 - val_accuracy: 0.2868\n",
      "Epoch 158/200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 2.9585 - accuracy: 0.2919 - val_loss: 3.0149 - val_accuracy: 0.2868\n",
      "Epoch 159/200\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 2.9707 - accuracy: 0.2828 - val_loss: 3.0112 - val_accuracy: 0.2886\n",
      "Epoch 160/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9635 - accuracy: 0.2905 - val_loss: 3.0074 - val_accuracy: 0.2924\n",
      "Epoch 161/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9696 - accuracy: 0.2829 - val_loss: 3.0037 - val_accuracy: 0.2924\n",
      "Epoch 162/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9678 - accuracy: 0.2793 - val_loss: 3.0000 - val_accuracy: 0.2924\n",
      "Epoch 163/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9623 - accuracy: 0.2738 - val_loss: 2.9963 - val_accuracy: 0.2942\n",
      "Epoch 164/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9465 - accuracy: 0.2866 - val_loss: 2.9927 - val_accuracy: 0.2961\n",
      "Epoch 165/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9399 - accuracy: 0.2905 - val_loss: 2.9890 - val_accuracy: 0.2980\n",
      "Epoch 166/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9582 - accuracy: 0.2815 - val_loss: 2.9853 - val_accuracy: 0.2980\n",
      "Epoch 167/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9568 - accuracy: 0.2812 - val_loss: 2.9818 - val_accuracy: 0.2961\n",
      "Epoch 168/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9356 - accuracy: 0.2812 - val_loss: 2.9781 - val_accuracy: 0.2980\n",
      "Epoch 169/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9448 - accuracy: 0.2804 - val_loss: 2.9745 - val_accuracy: 0.2998\n",
      "Epoch 170/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9249 - accuracy: 0.2918 - val_loss: 2.9710 - val_accuracy: 0.2998\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9269 - accuracy: 0.2909 - val_loss: 2.9674 - val_accuracy: 0.2998\n",
      "Epoch 172/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9365 - accuracy: 0.2779 - val_loss: 2.9638 - val_accuracy: 0.2998\n",
      "Epoch 173/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9359 - accuracy: 0.2794 - val_loss: 2.9603 - val_accuracy: 0.2998\n",
      "Epoch 174/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9148 - accuracy: 0.2899 - val_loss: 2.9568 - val_accuracy: 0.2980\n",
      "Epoch 175/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9235 - accuracy: 0.2759 - val_loss: 2.9533 - val_accuracy: 0.2980\n",
      "Epoch 176/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.9212 - accuracy: 0.2858 - val_loss: 2.9498 - val_accuracy: 0.2980\n",
      "Epoch 177/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8993 - accuracy: 0.2896 - val_loss: 2.9463 - val_accuracy: 0.2980\n",
      "Epoch 178/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8938 - accuracy: 0.2881 - val_loss: 2.9428 - val_accuracy: 0.2980\n",
      "Epoch 179/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8883 - accuracy: 0.2950 - val_loss: 2.9394 - val_accuracy: 0.2961\n",
      "Epoch 180/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8998 - accuracy: 0.2817 - val_loss: 2.9359 - val_accuracy: 0.2980\n",
      "Epoch 181/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8887 - accuracy: 0.2964 - val_loss: 2.9325 - val_accuracy: 0.2980\n",
      "Epoch 182/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8896 - accuracy: 0.2888 - val_loss: 2.9291 - val_accuracy: 0.2998\n",
      "Epoch 183/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8976 - accuracy: 0.2934 - val_loss: 2.9257 - val_accuracy: 0.2998\n",
      "Epoch 184/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8922 - accuracy: 0.2972 - val_loss: 2.9224 - val_accuracy: 0.3017\n",
      "Epoch 185/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8829 - accuracy: 0.2958 - val_loss: 2.9190 - val_accuracy: 0.3035\n",
      "Epoch 186/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8761 - accuracy: 0.2845 - val_loss: 2.9156 - val_accuracy: 0.3035\n",
      "Epoch 187/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8656 - accuracy: 0.2934 - val_loss: 2.9123 - val_accuracy: 0.3054\n",
      "Epoch 188/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8879 - accuracy: 0.2884 - val_loss: 2.9090 - val_accuracy: 0.3054\n",
      "Epoch 189/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8427 - accuracy: 0.3061 - val_loss: 2.9056 - val_accuracy: 0.3054\n",
      "Epoch 190/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8700 - accuracy: 0.2937 - val_loss: 2.9023 - val_accuracy: 0.3091\n",
      "Epoch 191/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8742 - accuracy: 0.2999 - val_loss: 2.8990 - val_accuracy: 0.3110\n",
      "Epoch 192/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8739 - accuracy: 0.2905 - val_loss: 2.8957 - val_accuracy: 0.3110\n",
      "Epoch 193/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8717 - accuracy: 0.2992 - val_loss: 2.8925 - val_accuracy: 0.3110\n",
      "Epoch 194/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8525 - accuracy: 0.2903 - val_loss: 2.8892 - val_accuracy: 0.3110\n",
      "Epoch 195/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8334 - accuracy: 0.2998 - val_loss: 2.8860 - val_accuracy: 0.3110\n",
      "Epoch 196/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8606 - accuracy: 0.2829 - val_loss: 2.8827 - val_accuracy: 0.3110\n",
      "Epoch 197/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8445 - accuracy: 0.2911 - val_loss: 2.8795 - val_accuracy: 0.3128\n",
      "Epoch 198/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8508 - accuracy: 0.2878 - val_loss: 2.8763 - val_accuracy: 0.3128\n",
      "Epoch 199/200\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 2.8477 - accuracy: 0.2882 - val_loss: 2.8731 - val_accuracy: 0.3128\n",
      "Epoch 200/200\n",
      " 33/103 [========>.....................] - ETA: 0s - loss: 2.8420 - accuracy: 0.3034"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-efaadb773dfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m    validation_data=(validation_data, validation_labels))\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_model_weights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hades/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hades/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hades/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hades/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hades/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hades/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/home/hades/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#This is the best model we found. For additional models, check out I_notebook.ipynb\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPool2D \n",
    "from tensorflow.keras.layers import Convolution2D,MaxPooling2D,Dropout,Dense,Flatten,BatchNormalization,Conv2D\n",
    "start = datetime.datetime.now()\n",
    "#print(train_data[2].shape[1])\n",
    "# model = Sequential() \n",
    "# #model.add(Flatten(input_shape=train_data.shape[1:])) \n",
    "# model.add(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.5)) \n",
    "# model.add(Dense(50, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.3)) \n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#    optimizer=optimizers.Adam(lr=1e-4),\n",
    "#    metrics=['acc'])\n",
    "\n",
    "# model.add(Dense(200, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.5)) \n",
    "# model.add(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(50, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#    optimizer=optimizers.Adam(lr=1e-4),\n",
    "# #    metrics=['acc'])\n",
    "\n",
    "###########\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "# model = Sequential() \n",
    "# model.add(BatchNormalization()) \n",
    "# model.add(Dropout(0.7)) \n",
    "# model.add(Dense(512)) \n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation(swish_act))\n",
    "\n",
    "# model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#    optimizer=optimizers.Adam(lr=1e-4),\n",
    "#    metrics=['acc'])\n",
    "######################################################################\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(tensorflow.keras.layers.Dense(1024, activation='relu'))\n",
    "# model.add(tensorflow.keras.layers.Dense(1024, activation='relu'))\n",
    "# model.add(tensorflow.keras.layers.Dense(512, activation='relu'))\n",
    "\n",
    "#model.add(BatchNormalization()) \n",
    "#model.add(Dropout(0.7)) \n",
    "#model.add(Dense(512)) \n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation(swish_act))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(128)) \n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation(swish_act))\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "   optimizer=optimizers.Adam(lr=1e-4),\n",
    "   metrics=['acc'])\n",
    "\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# # building 2 fully connected layer \n",
    "# #x = model.output\n",
    "\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Dropout(0.7)(x)\n",
    "\n",
    "# x = Dense(512)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation(swish_act)(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "\n",
    "# x = Dense(128)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation(swish_act)(x)\n",
    "\n",
    "# output layer\n",
    "#predictions = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "#model_final = Model(inputs = model.input, outputs = predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(0.0001),\n",
    "              metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, verbose=1,)\n",
    "mcp_save = ModelCheckpoint('EnetB0_CIFAR10_TL.h5', save_best_only=True, monitor='val_acc')\n",
    "\n",
    "history = model.fit(train_data, train_labels, \n",
    "   epochs=200,\n",
    "   batch_size=batch_size, \n",
    "   validation_data=(validation_data, validation_labels))\n",
    "\n",
    "model.save_weights(top_model_weights_path)\n",
    "(eval_loss, eval_accuracy) = model.evaluate( \n",
    "    validation_data, validation_labels, batch_size=batch_size,     verbose=1)\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
    "print('[INFO] Loss: {}'.format(eval_loss)) \n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(train_data, train_labels, \n",
    "   epochs=10,\n",
    "   batch_size=batch_size, \n",
    "   validation_data=(validation_data, validation_labels))\n",
    "\n",
    "model.save_weights(top_model_weights_path)\n",
    "(eval_loss, eval_accuracy) = model.evaluate( \n",
    "    validation_data, validation_labels, batch_size=batch_size,     verbose=1)\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
    "print('[INFO] Loss: {}'.format(eval_loss)) \n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# y_predict=model.predict(validation_data)\n",
    "# y_predict= np.argmax(y_predict, axis=1)\n",
    "# #print(y_predict)\n",
    "# y_true=np.argmax(validation_labels, axis=1)\n",
    "# #print(y_true)\n",
    "# print(classification_report(y_predict,y_true))\n",
    "# y_predict=list(y_predict)\n",
    "# y_true=list(y_true)\n",
    "# with open(\"Mobilenet_DataAug.txt\",\"w+\") as f:\n",
    "#     f.write(str([y_predict,y_true]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes=[]\n",
    "for x in range(len(y_true)):\n",
    "    if y_true[x]!=y_predict[x]:\n",
    "    #print(y_true[x], y_predict[x])\n",
    "        indexes.append(x)\n",
    "print(len(indexes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# dir_hapus=\"/media/hades/DISK 0 - Drive 3/Fish/Test7/Test/\"\n",
    "# for i in indexes[0:-20]:\n",
    "#     print(list_file[i])\n",
    "#     shutil.move(dir_hapus+list_file[i],\"/media/hades/DISK 0 - Drive 3/Fish/Test7/Test_Backup/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphing our training and validation\n",
    "#print(history.history)\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.ylabel('accuracy') \n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.ylabel('loss') \n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil=os.listdir(\"/media/hades/DISK 0 - Drive 3/Fish/Test7/Test/\")\n",
    "\n",
    "consumable=pd.read_csv(\"Commercial.txt\",delimiter=r\"\\t+\")\n",
    "consumable2=pd.read_csv(\"Game_Fishes.txt\",delimiter=r\"\\t+\")\n",
    "\n",
    "unconsumable=pd.read_csv(\"Dangerous.txt\",delimiter=r\"\\t+\")\n",
    "\n",
    "\n",
    "unconsumable_final=[]\n",
    "consumable_final=[]\n",
    "consumable2_final=[]\n",
    "\n",
    "for x in consumable['Species']:\n",
    "    x=x.replace(' ','_')\n",
    "    consumable_final.append(x)\n",
    "for x in unconsumable['Species']:\n",
    "    x=x.replace(' ','_')\n",
    "    unconsumable_final.append(x)\n",
    "for x in consumable2['Species']:\n",
    "    x=x.replace(' ','_')\n",
    "    consumable2_final.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sort_Tuple(tup):  \n",
    "  \n",
    "    # reverse = None (Sorts in Ascending order)  \n",
    "    # key is set to sort using second element of  \n",
    "    # sublist lambda has been used  \n",
    "    tup.sort(key = lambda x: x[2],reverse=True)  \n",
    "    return tup  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_image(file_path):\n",
    "   #print('[INFO] loading and preprocessing image…') \n",
    "   image = load_img(file_path, target_size=(224, 224)) \n",
    "   image = img_to_array(image) \n",
    "   image = np.expand_dims(image, axis=0)\n",
    "   image /= 255. \n",
    "   return image\n",
    "def test_single_image(path):\n",
    "  animals = os.listdir('/media/hades/DISK 0 - Drive 3/Fish/Test7/Train')\n",
    "  animals=sorted(animals)\n",
    "  images = read_image(path)\n",
    "  time.sleep(.5)\n",
    "  bt_prediction = vgg16.predict(images) \n",
    "  preds = model.predict_proba(bt_prediction)\n",
    "  #print(\"preds:\",preds)\n",
    "  finalpercent=Sort_Tuple(list(zip(range(0,667), animals , preds[0])))\n",
    "  #print(finalpercent)\n",
    "  #for idx, animal, x in zip(range(0,10), animals , preds[0]):\n",
    "   #print('ID: {}, Label: {} {}%'.format(idx, animal, round(x*100,2) ))\n",
    "    \n",
    "  #print('Final Decision:')\n",
    "  time.sleep(.5)\n",
    "  for x in range(3):\n",
    "   #print('.'*(x+1))\n",
    "   time.sleep(.2)\n",
    "  class_predicted = model.predict_classes(bt_prediction)\n",
    "  class_dictionary = generator_top.class_indices \n",
    "  inv_map = {v: k for k, v in class_dictionary.items()} \n",
    "  #print(\"PRED:\",class_dictionary.items())\n",
    "  finalpred='ID: {}, Label: {}'.format(class_predicted[0],  inv_map[class_predicted[0]])\n",
    "  finalpred2=[]\n",
    "  for x,y in enumerate(finalpercent[:5]):\n",
    "        finalpred2.append('ID: {}, Label: {} {}%'.format(y[0],y[1],y[2]))\n",
    "  #print(finalpred2)\n",
    "  #print('ID: {}, Label: {}'.format(class_predicted[0],  inv_map[class_predicted[0]])) \n",
    "  return finalpred,finalpred2,finalpercent\n",
    "\n",
    "path = '/media/hades/DISK 0 - Drive 3/Fish/Test7/Test/Naso/Naso_unicornis_0060.jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(species):\n",
    "    nama_species=species[0][1].replace(\"_\",' ')\n",
    "    print(\"namaSP:\",species[0][1])\n",
    "    pd.set_option('display.max_rows', unconsumable.shape[0]+1)\n",
    "    print(unconsumable)\n",
    "    if species[0][1] in consumable_final:\n",
    "        row=consumable.loc[consumable['Species'] == nama_species]\n",
    "        row_list=[]\n",
    "        \n",
    "        print(\"Tes1:\",row['Species'])\n",
    "        row_list.append(\"CONSUMABLE\")\n",
    "        row_list.append(\"Species Name: %s\" %row['Species'].values[0])\n",
    "        row_list.append(\"Order: %s\" %row['Order'].values[0])\n",
    "        row_list.append(\"Family: %s\" %row['Family'].values[0])\n",
    "        row_list.append(\"Foreign Name: %s\" %row['FishBase name'].values[0])\n",
    "        row_list.append(\"Local Name: %s\" %row['Name'].values[0])\n",
    "        row_list.append(\"Description: %s\" %row['Use'].values[0])\n",
    "    elif species[0][1] in consumable2_final:\n",
    "        row=consumable2.loc[consumable2['Species'] == nama_species]\n",
    "        row_list=[]\n",
    "        \n",
    "        print(\"Tes2:\",row['Species'])\n",
    "        row_list.append(\"CONSUMABLE\")\n",
    "        row_list.append(\"Species Name: %s\" %row['Species'].values[0])\n",
    "        row_list.append(\"Order: %s\" %row['Order'].values[0])\n",
    "        row_list.append(\"Family: %s\" %row['Family'].values[0])\n",
    "        row_list.append(\"Foreign Name: %s\" %row['FishBase name'].values[0])\n",
    "        row_list.append(\"Local Name: %s\" %row['Name'].values[0])\n",
    "        row_list.append(\"Description: %s\" %row['No'].values[0])\n",
    "    else:\n",
    "        print(\"ASDDDDDDDDDDDDDDDD\",nama_species)\n",
    "        row=unconsumable.loc[unconsumable['Species'] == nama_species]\n",
    "        row_list=[]\n",
    "        \n",
    "        print(\"Tes3:\",row['Species'])\n",
    "        row_list.append(\"UNCONSUMABLE\")\n",
    "        row_list.append(\"Species Name: %s\" %row['Species'].values[0])\n",
    "        row_list.append(\"Order: %s\" %row['Order'].values[0])\n",
    "        row_list.append(\"Family: %s\" %row['Family'].values[0])\n",
    "        row_list.append(\"Foreign Name: %s\" %row['FishBase name'].values[0])\n",
    "        row_list.append(\"Local Name: %s\" %row['Name'].values[0])\n",
    "        row_list.append(\"Description: %s\" %row['Danger'].values[0])\n",
    "        \n",
    "    return row_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "%matplotlib tk\n",
    "\n",
    "img = mpimg.imread(path)\n",
    "plt.imshow(img)\n",
    "a,b,c=test_single_image(path)\n",
    "print(\"HH\",b)\n",
    "#print(get_data(c))\n",
    "textstr = '\\n'.join((\n",
    "    r'Classification Result',\n",
    "        r'',\n",
    "    r'',\n",
    "    r'Final Result:',\n",
    "        r'',\n",
    "        r'%s'%a,\n",
    "        r'',\n",
    "        r'DEBUG: ',\n",
    "        r'',\n",
    "        r'%s'%b[0],\n",
    "        r'%s'%b[1],\n",
    "        r'%s'%b[2],\n",
    "        r'%s'%b[3],\n",
    "        r'%s'%b[4]\n",
    "    ))\n",
    "d=get_data(c)\n",
    "#print(get_data(c))\n",
    "#print(get_data(c))\n",
    "textstr2 = '\\n'.join((\n",
    "        r'%s'%d[0],\n",
    "        r'Details',\n",
    "        r'',\n",
    "        r'%s'%d[1],\n",
    "        r'%s'%d[2],\n",
    "        r'%s'%d[3],\n",
    "        r'%s'%d[4],\n",
    "        r'%s'%d[5],\n",
    "        r'%s'%d[6],\n",
    "        \n",
    "    ))\n",
    "plt.xlabel(' ')\n",
    "plt.ylabel(' ')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "plt.subplots_adjust(left=0.35)\n",
    "\n",
    "plt.text(0.05, 0.95, textstr, transform=plt.gcf().transFigure, fontsize=10,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.text(0.05, 0.42, textstr2, transform=plt.gcf().transFigure, fontsize=10,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
