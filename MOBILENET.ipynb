{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/hades/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hades/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hades/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hades/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hades/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hades/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import numpy as np \n",
    "import itertools\n",
    "import keras\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img \n",
    "from keras.models import Sequential \n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPool2D \n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Flatten\n",
    "from keras import applications \n",
    "from keras.utils.np_utils import to_categorical \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "import efficientnet.keras as enet\n",
    "import keras_applications as app\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import Model\n",
    "\n",
    "#%matplotlib inline\n",
    "import math \n",
    "import datetime\n",
    "import time\n",
    "CUDA_VISIBLE_DEVICES = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Default dimensions we found online\n",
    "img_width, img_height = 224, 224 \n",
    " \n",
    "#Create a bottleneck file\n",
    "top_model_weights_path = \"bottleneck_fc_model_50_50.h5\"\n",
    "# loading up our datasets\n",
    "train_data_dir = '/media/hades/DISK 0 - Drive 3/Fish/Test7/Train' \n",
    "validation_data_dir = '/media/hades/DISK 0 - Drive 3/Fish/Test7/Validate' \n",
    "test_data_dir = '/media/hades/DISK 0 - Drive 3/Fish/Test7/Test'\n",
    " \n",
    "# number of epochs to train top model \n",
    "epochs = 50 #this has been changed after multiple model run \n",
    "# batch size used by flow_from_directory and predict_generator \n",
    "batch_size = 50 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAS\n",
      "WARNING:tensorflow:From /home/hades/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hades/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hades/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hades/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hades/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hades/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import efficientnet.keras as enet\n",
    "\n",
    "#Loading vgc16 model\n",
    "print(\"HAS\")\n",
    "vgg16 = app.mobilenet_v2.MobileNetV2(utils = keras.utils, models=keras.models, layers = keras.layers, backend = keras.backend, include_top=False, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "#vgg16 = app.vgg16.VGG16(utils = keras.utils, models=keras.models, layers = keras.layers, backend = keras.backend, include_top=False, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "#vgg16=app.resnet.ResNet50(utils = keras.utils, models=keras.models, layers = keras.layers, include_top=False,input_shape=(224,224,3),weights='imagenet',pooling='avg', backend = keras.backend)\n",
    "#vgg16 = app.MobileNet(include_top=False, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                             rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "#needed to create the bottleneck .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASD1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/hades/DISK 0 - Drive 3/Fish/Test7/Train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3dae28fff52d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     shuffle=False) \n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnb_train_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         )\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/hades/DISK 0 - Drive 3/Fish/Test7/Train'"
     ]
    }
   ],
   "source": [
    "#__this can take an hour and half to run so only run it once. \n",
    "#once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__\n",
    "start = datetime.datetime.now()\n",
    "print(\"ASD1\")\n",
    "generator = datagen.flow_from_directory( \n",
    "    train_data_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode=None, \n",
    "    shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator.filenames) \n",
    "num_classes = len(generator.class_indices) \n",
    " \n",
    "predict_size_train = int(math.ceil(nb_train_samples / batch_size)) \n",
    " \n",
    "bottleneck_features_train = vgg16.predict_generator(generator, predict_size_train) \n",
    "vgg16.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "np.save('bottleneck_features_train_50_50.npy', bottleneck_features_train)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__this can take an hour and half to run so only run it once. \n",
    "#once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__\n",
    "start = datetime.datetime.now()\n",
    " \n",
    "generator = datagen.flow_from_directory( \n",
    "    test_data_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode=None, \n",
    "    shuffle=False) \n",
    " \n",
    "nb_validation_samples = len(generator.filenames) \n",
    "num_classes = len(generator.class_indices) \n",
    " \n",
    "predict_size_validation = int(math.ceil(nb_validation_samples / batch_size)) \n",
    " \n",
    "bottleneck_features_validation = vgg16.predict_generator(generator, predict_size_validation) \n",
    " \n",
    "np.save('bottleneck_features_validation_50_50.npy', bottleneck_features_validation)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "generator_top = datagen.flow_from_directory( \n",
    "   train_data_dir, \n",
    "   target_size=(img_width, img_height), \n",
    "   batch_size=batch_size, \n",
    "   class_mode='categorical', \n",
    "   shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator_top.filenames) \n",
    "num_classes = len(generator_top.class_indices) \n",
    " \n",
    "# load the bottleneck features saved earlier \n",
    "train_data = np.load('bottleneck_features_train_50_50.npy') \n",
    " \n",
    "# get the class labels for the training data, in the original order \n",
    "train_labels = generator_top.classes \n",
    " \n",
    "# convert the training labels to categorical vectors \n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "generator_top = datagen.flow_from_directory( \n",
    "   test_data_dir, \n",
    "   target_size=(img_width, img_height), \n",
    "   batch_size=batch_size, \n",
    "   class_mode='categorical', \n",
    "   shuffle=False) \n",
    " \n",
    "nb_validation_samples = len(generator_top.filenames) \n",
    "num_classes = len(generator_top.class_indices) \n",
    " \n",
    "# load the bottleneck features saved earlier \n",
    "validation_data = np.load('bottleneck_features_validation_50_50.npy') \n",
    " \n",
    "# get the class labels for the training data, in the original order \n",
    "validation_labels = generator_top.classes \n",
    " \n",
    "# convert the training labels to categorical vectors \n",
    "validation_labels = to_categorical(validation_labels, num_classes=num_classes)\n",
    "\n",
    "#validation_data = validation_data.reshape((-1, 224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swish defination\n",
    "from keras.backend import sigmoid\n",
    "\n",
    "class SwishActivation(Activation):\n",
    "    \n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(SwishActivation, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'swish_act'\n",
    "\n",
    "def swish_act(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "get_custom_objects().update({'swish_act': SwishActivation(swish_act)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This is the best model we found. For additional models, check out I_notebook.ipynb\n",
    "from keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPool2D \n",
    "from keras.layers import Convolution2D,MaxPooling2D,Dropout,Dense,Flatten,BatchNormalization,Conv2D\n",
    "start = datetime.datetime.now()\n",
    "#print(train_data[2].shape[1])\n",
    "# model = Sequential() \n",
    "# #model.add(Flatten(input_shape=train_data.shape[1:])) \n",
    "# model.add(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.5)) \n",
    "# model.add(Dense(50, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.3)) \n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#    optimizer=optimizers.Adam(lr=1e-4),\n",
    "#    metrics=['acc'])\n",
    "\n",
    "# model.add(Dense(200, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.5)) \n",
    "# model.add(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(50, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#    optimizer=optimizers.Adam(lr=1e-4),\n",
    "# #    metrics=['acc'])\n",
    "\n",
    "\n",
    "model = Sequential() \n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dropout(0.3)) \n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Activation(swish_act))\n",
    "model.add(Dropout(0.1)) \n",
    "model.add(Dense(64)) \n",
    "model.add(BatchNormalization()) \n",
    "model.add(Activation(swish_act))\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#    optimizer=optimizers.Adam(lr=1e-4),\n",
    "#    metrics=['acc'])\n",
    "\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# # building 2 fully connected layer \n",
    "# #x = model.output\n",
    "\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Dropout(0.7)(x)\n",
    "\n",
    "# x = Dense(512)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation(swish_act)(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "\n",
    "# x = Dense(128)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation(swish_act)(x)\n",
    "\n",
    "# output layer\n",
    "#predictions = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "#model_final = Model(inputs = model.input, outputs = predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(0.0001),\n",
    "              metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=10, verbose=1,)\n",
    "#mcp_save = ModelCheckpoint('EnetB0_CIFAR10_TL.h5', save_best_only=True, monitor='val_acc')\n",
    "\n",
    "history = model.fit(train_data, train_labels, \n",
    "   epochs=1000,\n",
    "   batch_size=batch_size, \n",
    "   validation_data=(validation_data, validation_labels), callbacks=[reduce_lr])\n",
    "\n",
    "model.save_weights(top_model_weights_path)\n",
    "(eval_loss, eval_accuracy) = model.evaluate( \n",
    "    validation_data, validation_labels, batch_size=batch_size,     verbose=1)\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
    "print('[INFO] Loss: {}'.format(eval_loss)) \n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphing our training and validation\n",
    "#print(history.history)\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.ylabel('accuracy') \n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.ylabel('loss') \n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil=os.listdir(\"/media/hades/DISK 1 - Drive 2/Fish/Test/Test/\")\n",
    "\n",
    "consumable=pd.read_csv(\"Commercial.txt\",delimiter=r\"\\t+\")\n",
    "consumable2=pd.read_csv(\"Game_Fishes.txt\",delimiter=r\"\\t+\")\n",
    "\n",
    "unconsumable=pd.read_csv(\"Dangerous.txt\",delimiter=r\"\\t+\")\n",
    "\n",
    "\n",
    "unconsumable_final=[]\n",
    "consumable_final=[]\n",
    "consumable2_final=[]\n",
    "\n",
    "for x in consumable['Species']:\n",
    "    x=x.replace(' ','_')\n",
    "    consumable_final.append(x)\n",
    "for x in unconsumable['Species']:\n",
    "    x=x.replace(' ','_')\n",
    "    unconsumable_final.append(x)\n",
    "for x in consumable2['Species']:\n",
    "    x=x.replace(' ','_')\n",
    "    consumable2_final.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sort_Tuple(tup):  \n",
    "  \n",
    "    # reverse = None (Sorts in Ascending order)  \n",
    "    # key is set to sort using second element of  \n",
    "    # sublist lambda has been used  \n",
    "    tup.sort(key = lambda x: x[2],reverse=True)  \n",
    "    return tup  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_image(file_path):\n",
    "   #print('[INFO] loading and preprocessing image…') \n",
    "   image = load_img(file_path, target_size=(224, 224)) \n",
    "   image = img_to_array(image) \n",
    "   image = np.expand_dims(image, axis=0)\n",
    "   image /= 255. \n",
    "   return image\n",
    "def test_single_image(path):\n",
    "  animals = os.listdir('/media/hades/DISK 1 - Drive 2/Fish/Test/Train')\n",
    "  animals=sorted(animals)\n",
    "  images = read_image(path)\n",
    "  time.sleep(.5)\n",
    "  bt_prediction = vgg16.predict(images) \n",
    "  preds = model.predict_proba(bt_prediction)\n",
    "  #print(\"preds:\",preds)\n",
    "  finalpercent=Sort_Tuple(list(zip(range(0,667), animals , preds[0])))\n",
    "  #print(finalpercent)\n",
    "  #for idx, animal, x in zip(range(0,10), animals , preds[0]):\n",
    "   #print('ID: {}, Label: {} {}%'.format(idx, animal, round(x*100,2) ))\n",
    "    \n",
    "  #print('Final Decision:')\n",
    "  time.sleep(.5)\n",
    "  for x in range(3):\n",
    "   #print('.'*(x+1))\n",
    "   time.sleep(.2)\n",
    "  class_predicted = model.predict_classes(bt_prediction)\n",
    "  class_dictionary = generator_top.class_indices \n",
    "  inv_map = {v: k for k, v in class_dictionary.items()} \n",
    "  #print(\"PRED:\",class_dictionary.items())\n",
    "  finalpred='ID: {}, Label: {}'.format(class_predicted[0],  inv_map[class_predicted[0]])\n",
    "  finalpred2=[]\n",
    "  for x,y in enumerate(finalpercent[:5]):\n",
    "        finalpred2.append('ID: {}, Label: {} {}%'.format(y[0],y[1],y[2]))\n",
    "  #print(finalpred2)\n",
    "  #print('ID: {}, Label: {}'.format(class_predicted[0],  inv_map[class_predicted[0]])) \n",
    "  return finalpred,finalpred2,finalpercent\n",
    "\n",
    "path = '/media/hades/DISK 1 - Drive 2/Fish/Test/Test/Naso_lituratus/Naso_lituratus_0063.jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(species):\n",
    "    nama_species=species[0][1].replace(\"_\",' ')\n",
    "    print(\"namaSP:\",species[0][1])\n",
    "    pd.set_option('display.max_rows', unconsumable.shape[0]+1)\n",
    "    print(unconsumable)\n",
    "    if species[0][1] in consumable_final:\n",
    "        row=consumable.loc[consumable['Species'] == nama_species]\n",
    "        row_list=[]\n",
    "        \n",
    "        print(\"Tes1:\",row['Species'])\n",
    "        row_list.append(\"CONSUMABLE\")\n",
    "        row_list.append(\"Species Name: %s\" %row['Species'].values[0])\n",
    "        row_list.append(\"Order: %s\" %row['Order'].values[0])\n",
    "        row_list.append(\"Family: %s\" %row['Family'].values[0])\n",
    "        row_list.append(\"Foreign Name: %s\" %row['FishBase name'].values[0])\n",
    "        row_list.append(\"Local Name: %s\" %row['Name'].values[0])\n",
    "        row_list.append(\"Description: %s\" %row['Use'].values[0])\n",
    "    elif species[0][1] in consumable2_final:\n",
    "        row=consumable2.loc[consumable2['Species'] == nama_species]\n",
    "        row_list=[]\n",
    "        \n",
    "        print(\"Tes2:\",row['Species'])\n",
    "        row_list.append(\"CONSUMABLE\")\n",
    "        row_list.append(\"Species Name: %s\" %row['Species'].values[0])\n",
    "        row_list.append(\"Order: %s\" %row['Order'].values[0])\n",
    "        row_list.append(\"Family: %s\" %row['Family'].values[0])\n",
    "        row_list.append(\"Foreign Name: %s\" %row['FishBase name'].values[0])\n",
    "        row_list.append(\"Local Name: %s\" %row['Name'].values[0])\n",
    "        row_list.append(\"Description: %s\" %row['No'].values[0])\n",
    "    else:\n",
    "        print(\"ASDDDDDDDDDDDDDDDD\",nama_species)\n",
    "        row=unconsumable.loc[unconsumable['Species'] == nama_species]\n",
    "        row_list=[]\n",
    "        \n",
    "        print(\"Tes3:\",row['Species'])\n",
    "        row_list.append(\"UNCONSUMABLE\")\n",
    "        row_list.append(\"Species Name: %s\" %row['Species'].values[0])\n",
    "        row_list.append(\"Order: %s\" %row['Order'].values[0])\n",
    "        row_list.append(\"Family: %s\" %row['Family'].values[0])\n",
    "        row_list.append(\"Foreign Name: %s\" %row['FishBase name'].values[0])\n",
    "        row_list.append(\"Local Name: %s\" %row['Name'].values[0])\n",
    "        row_list.append(\"Description: %s\" %row['Danger'].values[0])\n",
    "        \n",
    "    return row_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "%matplotlib tk\n",
    "\n",
    "img = mpimg.imread(path)\n",
    "plt.imshow(img)\n",
    "a,b,c=test_single_image(path)\n",
    "print(get_data(c))\n",
    "textstr = '\\n'.join((\n",
    "    r'Classification Result',\n",
    "        r'',\n",
    "    r'',\n",
    "    r'Final Result:',\n",
    "        r'',\n",
    "        r'%s'%a,\n",
    "        r'',\n",
    "        r'DEBUG: ',\n",
    "        r'',\n",
    "        r'%s'%b[0],\n",
    "        r'%s'%b[1],\n",
    "        r'%s'%b[2],\n",
    "        r'%s'%b[3],\n",
    "        r'%s'%b[4]\n",
    "    ))\n",
    "d=get_data(c)\n",
    "print(get_data(c))\n",
    "#print(get_data(c))\n",
    "textstr2 = '\\n'.join((\n",
    "        r'%s'%d[0],\n",
    "        r'Details',\n",
    "        r'',\n",
    "        r'%s'%d[1],\n",
    "        r'%s'%d[2],\n",
    "        r'%s'%d[3],\n",
    "        r'%s'%d[4],\n",
    "        r'%s'%d[5],\n",
    "        r'%s'%d[6],\n",
    "        \n",
    "    ))\n",
    "plt.xlabel(' ')\n",
    "plt.ylabel(' ')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "plt.subplots_adjust(left=0.35)\n",
    "\n",
    "plt.text(0.05, 0.95, textstr, transform=plt.gcf().transFigure, fontsize=10,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.text(0.05, 0.42, textstr2, transform=plt.gcf().transFigure, fontsize=10,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
