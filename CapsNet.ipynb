{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import numpy as np \n",
    "import itertools\n",
    "import tensorflow.keras\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPool2D \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization, Flatten\n",
    "from tensorflow.keras import applications \n",
    "from keras.utils.np_utils import to_categorical \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "import efficientnet.keras as enet\n",
    "import keras_applications as app\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#%matplotlib inline\n",
    "import math \n",
    "import datetime\n",
    "import time\n",
    "CUDA_VISIBLE_DEVICES = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Default dimensions we found online\n",
    "img_width, img_height = 224, 224 \n",
    " \n",
    "#Create a bottleneck file\n",
    "top_model_weights_path = \"bottleneck_fc_model_50_50.h5\"\n",
    "# loading up our datasets\n",
    "train_data_dir = '/media/hades/DISK 0 - Drive 3/Fish/Test7/Train' \n",
    "validation_data_dir = '/media/hades/DISK 0 - Drive 3/Fish/Test7/Validate' \n",
    "test_data_dir = '/media/hades/DISK 0 - Drive 3/Fish/Test7/Test'\n",
    " \n",
    "# number of epochs to train top model \n",
    "epochs = 50 #this has been changed after multiple model run \n",
    "# batch size used by flow_from_directory and predict_generator \n",
    "batch_size = 50 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAS\n",
      "HADOH\n"
     ]
    }
   ],
   "source": [
    "import efficientnet.keras as enet\n",
    "\n",
    "#Loading vgc16 model\n",
    "print(\"HAS\")\n",
    "#vgg16 = app.efficientnet.EfficientNetB7(utils = tensorflow.keras.utils, models=tensorflow.keras.models, layers = tensorflow.keras.layers, backend = tensorflow.keras.backend, include_top=False, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "#vgg16 = app.vgg16.VGG16(utils = keras.utils, models=keras.models, layers = keras.layers, backend = keras.backend, include_top=False, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "#vgg16 = applications.VGG16(include_top=False, weights=’imagenet’)\n",
    "#vgg16 = app.mobilenet_v3.MobileNetV3(utils = keras.utils, models=keras.models, layers = keras.layers, backend = keras.backend, include_top=False, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "#vgg16 = app.resnet50.ResNet50(utils = keras.utils, models=keras.models, layers = keras.layers, backend = keras.backend, include_top=False, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "vgg16=app.mobilenet.MobileNet(utils = tensorflow.keras.utils, models=tensorflow.keras.models, layers = tensorflow.keras.layers, backend = tensorflow.keras.backend, include_top=False, input_shape=(224,224,3), pooling='avg', weights='imagenet')\n",
    "datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "# datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "#       )\n",
    "print(\"HADOH\")\n",
    "#needed to create the bottleneck .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASD1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/hades/DISK 0 - Drive 3/Fish/Test7/Train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3dae28fff52d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     shuffle=False) \n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnb_train_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hades/.local/lib/python3.6/site-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m         interpolation=interpolation)\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m   def flow_from_dataframe(self,\n",
      "\u001b[0;32m/home/hades/.local/lib/python3.6/site-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hades/.local/lib/python3.6/site-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/hades/DISK 0 - Drive 3/Fish/Test7/Train'"
     ]
    }
   ],
   "source": [
    "#__this can take an hour and half to run so only run it once. \n",
    "#once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__\n",
    "start = datetime.datetime.now()\n",
    "print(\"ASD1\")\n",
    "generator = datagen.flow_from_directory( \n",
    "    train_data_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode=None, \n",
    "    shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator.filenames) \n",
    "num_classes = len(generator.class_indices) \n",
    " \n",
    "predict_size_train = int(math.ceil(nb_train_samples / batch_size)) \n",
    " \n",
    "bottleneck_features_train = vgg16.predict_generator(generator, predict_size_train) \n",
    "vgg16.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "np.save('bottleneck_features_train_50_50.npy', bottleneck_features_train)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__this can take an hour and half to run so only run it once. \n",
    "#once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__\n",
    "start = datetime.datetime.now()\n",
    " \n",
    "generator = datagen.flow_from_directory( \n",
    "    test_data_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode=None, \n",
    "    shuffle=False) \n",
    " \n",
    "nb_validation_samples = len(generator.filenames) \n",
    "num_classes = len(generator.class_indices) \n",
    " \n",
    "predict_size_validation = int(math.ceil(nb_validation_samples / batch_size)) \n",
    " \n",
    "bottleneck_features_validation = vgg16.predict_generator(generator, predict_size_validation) \n",
    " \n",
    "np.save('bottleneck_features_validation_50_50.npy', bottleneck_features_validation)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "generator_top = datagen.flow_from_directory( \n",
    "   train_data_dir, \n",
    "   target_size=(img_width, img_height), \n",
    "   batch_size=batch_size, \n",
    "   class_mode='categorical', \n",
    "   shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator_top.filenames) \n",
    "num_classes = len(generator_top.class_indices) \n",
    " \n",
    "# load the bottleneck features saved earlier \n",
    "train_data = np.load('bottleneck_features_train_50_50.npy') \n",
    " \n",
    "# get the class labels for the training data, in the original order \n",
    "train_labels = generator_top.classes \n",
    " \n",
    "# convert the training labels to categorical vectors \n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "generator_top = datagen.flow_from_directory( \n",
    "   test_data_dir, \n",
    "   target_size=(img_width, img_height), \n",
    "   batch_size=batch_size, \n",
    "   class_mode='categorical', \n",
    "   shuffle=False) \n",
    " \n",
    "nb_validation_samples = len(generator_top.filenames) \n",
    "num_classes = len(generator_top.class_indices) \n",
    " \n",
    "# load the bottleneck features saved earlier \n",
    "validation_data = np.load('bottleneck_features_validation_50_50.npy') \n",
    " \n",
    "# get the class labels for the training data, in the original order \n",
    "validation_labels = generator_top.classes \n",
    " \n",
    "# convert the training labels to categorical vectors \n",
    "validation_labels = to_categorical(validation_labels, num_classes=num_classes)\n",
    "\n",
    "#validation_data = validation_data.reshape((-1, 224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_file=generator_top.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swish defination\n",
    "from tensorflow.keras.backend import sigmoid\n",
    "\n",
    "class SwishActivation(Activation):\n",
    "    \n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(SwishActivation, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'swish_act'\n",
    "\n",
    "def swish_act(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation\n",
    "get_custom_objects().update({'swish_act': SwishActivation(swish_act)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This is the best model we found. For additional models, check out I_notebook.ipynb\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPool2D \n",
    "from tensorflow.keras.layers import Convolution2D,MaxPooling2D,Dropout,Dense,Flatten,BatchNormalization,Conv2D\n",
    "start = datetime.datetime.now()\n",
    "#print(train_data[2].shape[1])\n",
    "# model = Sequential() \n",
    "# #model.add(Flatten(input_shape=train_data.shape[1:])) \n",
    "# model.add(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.5)) \n",
    "# model.add(Dense(50, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.3)) \n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#    optimizer=optimizers.Adam(lr=1e-4),\n",
    "#    metrics=['acc'])\n",
    "\n",
    "# model.add(Dense(200, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.5)) \n",
    "# model.add(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(50, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#    optimizer=optimizers.Adam(lr=1e-4),\n",
    "# #    metrics=['acc'])\n",
    "\n",
    "\n",
    "model = Sequential() \n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dropout(0.7)) \n",
    "model.add(Dense(512)) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(swish_act))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(128)) \n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation(swish_act))\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "   optimizer=optimizers.Adam(lr=1e-4),\n",
    "   metrics=['acc'])\n",
    "\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# # building 2 fully connected layer \n",
    "# #x = model.output\n",
    "\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Dropout(0.7)(x)\n",
    "\n",
    "# x = Dense(512)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation(swish_act)(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "\n",
    "# x = Dense(128)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation(swish_act)(x)\n",
    "\n",
    "# output layer\n",
    "#predictions = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "#model_final = Model(inputs = model.input, outputs = predictions)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(0.0001),\n",
    "              metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, verbose=1,)\n",
    "mcp_save = ModelCheckpoint('EnetB0_CIFAR10_TL.h5', save_best_only=True, monitor='val_acc')\n",
    "\n",
    "history = model.fit(train_data, train_labels, \n",
    "   epochs=200,\n",
    "   batch_size=batch_size, \n",
    "   validation_data=(validation_data, validation_labels))\n",
    "\n",
    "model.save_weights(top_model_weights_path)\n",
    "(eval_loss, eval_accuracy) = model.evaluate( \n",
    "    validation_data, validation_labels, batch_size=batch_size,     verbose=1)\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
    "print('[INFO] Loss: {}'.format(eval_loss)) \n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(train_data, train_labels, \n",
    "   epochs=10,\n",
    "   batch_size=batch_size, \n",
    "   validation_data=(validation_data, validation_labels))\n",
    "\n",
    "model.save_weights(top_model_weights_path)\n",
    "(eval_loss, eval_accuracy) = model.evaluate( \n",
    "    validation_data, validation_labels, batch_size=batch_size,     verbose=1)\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
    "print('[INFO] Loss: {}'.format(eval_loss)) \n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_predict=model.predict(validation_data)\n",
    "y_predict= np.argmax(y_predict, axis=1)\n",
    "#print(y_predict)\n",
    "y_true=np.argmax(validation_labels, axis=1)\n",
    "#print(y_true)\n",
    "print(classification_report(y_predict,y_true))\n",
    "y_predict=list(y_predict)\n",
    "y_true=list(y_true)\n",
    "with open(\"Mobilenet_DataAug.txt\",\"w+\") as f:\n",
    "    f.write(str([y_predict,y_true]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes=[]\n",
    "for x in range(len(y_true)):\n",
    "    if y_true[x]!=y_predict[x]:\n",
    "    #print(y_true[x], y_predict[x])\n",
    "        indexes.append(x)\n",
    "print(len(indexes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# dir_hapus=\"/media/hades/DISK 0 - Drive 3/Fish/Test7/Test/\"\n",
    "# for i in indexes[0:-20]:\n",
    "#     print(list_file[i])\n",
    "#     shutil.move(dir_hapus+list_file[i],\"/media/hades/DISK 0 - Drive 3/Fish/Test7/Test_Backup/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphing our training and validation\n",
    "#print(history.history)\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.ylabel('accuracy') \n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.ylabel('loss') \n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil=os.listdir(\"/media/hades/DISK 0 - Drive 3/Fish/Test7/Test/\")\n",
    "\n",
    "consumable=pd.read_csv(\"Commercial.txt\",delimiter=r\"\\t+\")\n",
    "consumable2=pd.read_csv(\"Game_Fishes.txt\",delimiter=r\"\\t+\")\n",
    "\n",
    "unconsumable=pd.read_csv(\"Dangerous.txt\",delimiter=r\"\\t+\")\n",
    "\n",
    "\n",
    "unconsumable_final=[]\n",
    "consumable_final=[]\n",
    "consumable2_final=[]\n",
    "\n",
    "for x in consumable['Species']:\n",
    "    x=x.replace(' ','_')\n",
    "    consumable_final.append(x)\n",
    "for x in unconsumable['Species']:\n",
    "    x=x.replace(' ','_')\n",
    "    unconsumable_final.append(x)\n",
    "for x in consumable2['Species']:\n",
    "    x=x.replace(' ','_')\n",
    "    consumable2_final.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sort_Tuple(tup):  \n",
    "  \n",
    "    # reverse = None (Sorts in Ascending order)  \n",
    "    # key is set to sort using second element of  \n",
    "    # sublist lambda has been used  \n",
    "    tup.sort(key = lambda x: x[2],reverse=True)  \n",
    "    return tup  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_image(file_path):\n",
    "   #print('[INFO] loading and preprocessing image…') \n",
    "   image = load_img(file_path, target_size=(224, 224)) \n",
    "   image = img_to_array(image) \n",
    "   image = np.expand_dims(image, axis=0)\n",
    "   image /= 255. \n",
    "   return image\n",
    "def test_single_image(path):\n",
    "  animals = os.listdir('/media/hades/DISK 0 - Drive 3/Fish/Test7/Train')\n",
    "  animals=sorted(animals)\n",
    "  images = read_image(path)\n",
    "  time.sleep(.5)\n",
    "  bt_prediction = vgg16.predict(images) \n",
    "  preds = model.predict_proba(bt_prediction)\n",
    "  #print(\"preds:\",preds)\n",
    "  finalpercent=Sort_Tuple(list(zip(range(0,667), animals , preds[0])))\n",
    "  #print(finalpercent)\n",
    "  #for idx, animal, x in zip(range(0,10), animals , preds[0]):\n",
    "   #print('ID: {}, Label: {} {}%'.format(idx, animal, round(x*100,2) ))\n",
    "    \n",
    "  #print('Final Decision:')\n",
    "  time.sleep(.5)\n",
    "  for x in range(3):\n",
    "   #print('.'*(x+1))\n",
    "   time.sleep(.2)\n",
    "  class_predicted = model.predict_classes(bt_prediction)\n",
    "  class_dictionary = generator_top.class_indices \n",
    "  inv_map = {v: k for k, v in class_dictionary.items()} \n",
    "  #print(\"PRED:\",class_dictionary.items())\n",
    "  finalpred='ID: {}, Label: {}'.format(class_predicted[0],  inv_map[class_predicted[0]])\n",
    "  finalpred2=[]\n",
    "  for x,y in enumerate(finalpercent[:5]):\n",
    "        finalpred2.append('ID: {}, Label: {} {}%'.format(y[0],y[1],y[2]))\n",
    "  #print(finalpred2)\n",
    "  #print('ID: {}, Label: {}'.format(class_predicted[0],  inv_map[class_predicted[0]])) \n",
    "  return finalpred,finalpred2,finalpercent\n",
    "\n",
    "path = '/media/hades/DISK 0 - Drive 3/Fish/Test7/Test/Naso/Naso_unicornis_0060.jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(species):\n",
    "    nama_species=species[0][1].replace(\"_\",' ')\n",
    "    print(\"namaSP:\",species[0][1])\n",
    "    pd.set_option('display.max_rows', unconsumable.shape[0]+1)\n",
    "    print(unconsumable)\n",
    "    if species[0][1] in consumable_final:\n",
    "        row=consumable.loc[consumable['Species'] == nama_species]\n",
    "        row_list=[]\n",
    "        \n",
    "        print(\"Tes1:\",row['Species'])\n",
    "        row_list.append(\"CONSUMABLE\")\n",
    "        row_list.append(\"Species Name: %s\" %row['Species'].values[0])\n",
    "        row_list.append(\"Order: %s\" %row['Order'].values[0])\n",
    "        row_list.append(\"Family: %s\" %row['Family'].values[0])\n",
    "        row_list.append(\"Foreign Name: %s\" %row['FishBase name'].values[0])\n",
    "        row_list.append(\"Local Name: %s\" %row['Name'].values[0])\n",
    "        row_list.append(\"Description: %s\" %row['Use'].values[0])\n",
    "    elif species[0][1] in consumable2_final:\n",
    "        row=consumable2.loc[consumable2['Species'] == nama_species]\n",
    "        row_list=[]\n",
    "        \n",
    "        print(\"Tes2:\",row['Species'])\n",
    "        row_list.append(\"CONSUMABLE\")\n",
    "        row_list.append(\"Species Name: %s\" %row['Species'].values[0])\n",
    "        row_list.append(\"Order: %s\" %row['Order'].values[0])\n",
    "        row_list.append(\"Family: %s\" %row['Family'].values[0])\n",
    "        row_list.append(\"Foreign Name: %s\" %row['FishBase name'].values[0])\n",
    "        row_list.append(\"Local Name: %s\" %row['Name'].values[0])\n",
    "        row_list.append(\"Description: %s\" %row['No'].values[0])\n",
    "    else:\n",
    "        print(\"ASDDDDDDDDDDDDDDDD\",nama_species)\n",
    "        row=unconsumable.loc[unconsumable['Species'] == nama_species]\n",
    "        row_list=[]\n",
    "        \n",
    "        print(\"Tes3:\",row['Species'])\n",
    "        row_list.append(\"UNCONSUMABLE\")\n",
    "        row_list.append(\"Species Name: %s\" %row['Species'].values[0])\n",
    "        row_list.append(\"Order: %s\" %row['Order'].values[0])\n",
    "        row_list.append(\"Family: %s\" %row['Family'].values[0])\n",
    "        row_list.append(\"Foreign Name: %s\" %row['FishBase name'].values[0])\n",
    "        row_list.append(\"Local Name: %s\" %row['Name'].values[0])\n",
    "        row_list.append(\"Description: %s\" %row['Danger'].values[0])\n",
    "        \n",
    "    return row_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "%matplotlib tk\n",
    "\n",
    "img = mpimg.imread(path)\n",
    "plt.imshow(img)\n",
    "a,b,c=test_single_image(path)\n",
    "print(\"HH\",b)\n",
    "#print(get_data(c))\n",
    "textstr = '\\n'.join((\n",
    "    r'Classification Result',\n",
    "        r'',\n",
    "    r'',\n",
    "    r'Final Result:',\n",
    "        r'',\n",
    "        r'%s'%a,\n",
    "        r'',\n",
    "        r'DEBUG: ',\n",
    "        r'',\n",
    "        r'%s'%b[0],\n",
    "        r'%s'%b[1],\n",
    "        r'%s'%b[2],\n",
    "        r'%s'%b[3],\n",
    "        r'%s'%b[4]\n",
    "    ))\n",
    "d=get_data(c)\n",
    "#print(get_data(c))\n",
    "#print(get_data(c))\n",
    "textstr2 = '\\n'.join((\n",
    "        r'%s'%d[0],\n",
    "        r'Details',\n",
    "        r'',\n",
    "        r'%s'%d[1],\n",
    "        r'%s'%d[2],\n",
    "        r'%s'%d[3],\n",
    "        r'%s'%d[4],\n",
    "        r'%s'%d[5],\n",
    "        r'%s'%d[6],\n",
    "        \n",
    "    ))\n",
    "plt.xlabel(' ')\n",
    "plt.ylabel(' ')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "plt.subplots_adjust(left=0.35)\n",
    "\n",
    "plt.text(0.05, 0.95, textstr, transform=plt.gcf().transFigure, fontsize=10,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.text(0.05, 0.42, textstr2, transform=plt.gcf().transFigure, fontsize=10,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
